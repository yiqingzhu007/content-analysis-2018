{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Retreiving and Preparing Text for Machines\n",
    "\n",
    "This week, we begin by \"begging, borrowing and stealing\" text from several\n",
    "contexts of human communication (e.g., PDFs, HTML, Word) and preparing it for\n",
    "machines to \"read\" and analyze. This notebook outlines scraping text from the\n",
    "web, PDF and Word documents. Then we detail \"spidering\" or walking\n",
    "through hyperlinks to build samples of online content, and using APIs,\n",
    "Application Programming Interfaces, provided by webservices to access their\n",
    "content. Along the way, we will use regular expressions, outlined in the\n",
    "reading, to remove unwanted formatting and ornamentation. Finally, we discuss\n",
    "various text encodings, filtering and data structures in which text can be\n",
    "placed for analysis.\n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "import lucem_illud #pip install git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import requests #for http requests\n",
    "import bs4 #called `beautifulsoup4`, an html parser\n",
    "import pandas #gives us DataFrames\n",
    "import docx #reading MS doc files, install as `python-docx`\n",
    "\n",
    "#Stuff for pdfs\n",
    "#Install as `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#These come with Python\n",
    "import re #for regexs\n",
    "import urllib.parse #For joining urls\n",
    "import io #for making http requests look like files\n",
    "import json #For Tumblr API responses\n",
    "import os.path #For checking if files exist\n",
    "import os #For making directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be working on the following files/urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/Content%20Analysis%2018.pdf'\n",
    "example_docx = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/macs6000_connecting_to_midway.docx'\n",
    "example_docx_save = 'example.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Before we can start analyzing content we need to obtain it. Sometimes it will be\n",
    "provided to us from a pre-curated text archive, but sometimes we will need to\n",
    "download it. As a starting example we will attempt to download the wikipedia\n",
    "page on content analysis. The page is located at [https://en.wikipedia.org/wiki/\n",
    "Content_analysis](https://en.wikipedia.org/wiki/Content_analysis) so lets start\n",
    "with that.\n",
    "\n",
    "We can do this by making an HTTP GET request to that url, a GET request is\n",
    "simply a request to the server to provide the contents given by some url. The\n",
    "other request we will be using in this class is called a POST request and\n",
    "requests the server to take some content we provide. While the Python standard\n",
    "library does have the ability do make GET requests we will be using the\n",
    "[_requests_](http://docs.python-requests.org/en/master/) package as it is _'the\n",
    "only Non-GMO HTTP library for Python'_...also it provides a nicer interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get(wikipedia_content_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` means the server responded with what we asked for. If you get\n",
    "another number (e.g. 404) it likely means there was some kind of error, these\n",
    "codes are called HTTP response codes and a list of them can be found\n",
    "[here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). The response\n",
    "object contains all the data the server sent including the website's contents\n",
    "and the HTTP header. We are interested in the contents which we can access with\n",
    "the `.text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Content analysis - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n",
      "<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Content_analysis\",\"wgTitle\":\"Content analysis\",\"wgCurRevisionId\":819393184,\"wgRevisionId\":819393184,\"wgArticleId\":473317,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing expert attention with no reason or talk parameter\",\"Articles needing expert attention from April 2008\",\"All articles needing expert attention\",\"Sociology articles needing expert attention\",\"Media articles needing expert attention\",\"Articles that may be too long from January 2018\",\"All articles with un\n"
     ]
    }
   ],
   "source": [
    "wikiContentRequest = requests.get(wikipedia_content_analysis)\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we were looking for, because it is the start of the HTML that\n",
    "makes up the website. This is HTML and is meant to be read by computers. Luckily\n",
    "we have a computer to parse it for us. To do the parsing we will use [_Beautiful\n",
    "Soup_](https://www.crummy.com/software/BeautifulSoup/) which is a better parser\n",
    "than the one in the standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Content analysis - Wikipedia\n",
      "document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "(window.RLQ=window.RLQ||[]).push(functio\n"
     ]
    }
   ],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better but there's still random whitespace and we have more than just\n",
    "the text of the article. This is because what we requested is the whole webpage,\n",
    "not just the text for the article.\n",
    "\n",
    "We want to extract only the text we care about, and in order to do this we will\n",
    "need to inspect the html. One way to do this is simply to go to the website with\n",
    "a browser and use its inspection or view source tool. If javascript or other\n",
    "dynamic loading occurs on the page, however, it is likely that what Python\n",
    "receives is not what you will see, so we will need to inspect what Python\n",
    "receives. To do this we can save the html `requests` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open the file (`wikipedia_content_analysis.html`) we just created with\n",
    "a web browser. It should look sort of like the original but without the images\n",
    "and formatting.\n",
    "\n",
    "As there is very little standardization on structuring webpages, figuring out\n",
    "how best to extract what you want is an art. Looking at this page it looks like\n",
    "all the main textual content is inside `<p>`(paragraph) tags within the `<body>`\n",
    "tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Content analysis is a research method for studying documents and communication artifacts, which can be texts of various formats, pictures, audio or video. Social scientists use content analysis to quantify patterns in communication, in a replicable and systematic manner.[1] One of the key advantage of this research method is to analyse social phenomena in a non-invasive way, in contrast to simulating social experiences or collecting survey answers.\n",
      "Practices and philosophies of content analysis vary between scholarly communities. They all involve systematic reading or observation of texts or artifacts which are assigned labels (sometimes called codes) to indicate the presence of interesting, meaningful patterns.[2][3] After labeling a large set of media, a researcher is able to statistically estimate the proportions of patterns in the texts, as well as correlations between patterns.\n"
     ]
    }
   ],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the text from the page, split up by paragraph. If we wanted to\n",
    "get the section headers or references as well it would require a bit more work,\n",
    "but is doable.\n",
    "\n",
    "There is one more thing we might want to do before sending this text to be\n",
    "processed, remove the references indicators (`[2]`, `[3]` , etc). To do this we\n",
    "can use a short regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       paragraph-text\n",
      "0                                                    \n",
      "1   \\nContent analysis is a research method for st...\n",
      "2   Practices and philosophies of content analysis...\n",
      "3   Computers are increasingly used in content ana...\n",
      "4                                                    \n",
      "5                                                    \n",
      "6   Content analysis is best understood as a broad...\n",
      "7   The simplest and most objective form of conten...\n",
      "8   A further step in analysis is the distinction ...\n",
      "9   More generally, content analysis is research u...\n",
      "10  By having contents of communication available ...\n",
      "11  Robert Weber notes: \"To make valid inferences ...\n",
      "12  There are five types of texts in content analy...\n",
      "13  Over the years, content analysis has been appl...\n",
      "14  In recent times, particularly with the advent ...\n",
      "15  Quantitative content analysis has enjoyed a re...\n",
      "16  Recently, Arash Heydarian Pashakhanlou has arg...\n",
      "17  Content analysis can also be described as stud...\n",
      "18  The method of content analysis enables the res...\n",
      "19  Since the 1980s, content analysis has become a...\n",
      "20  The creation of coding frames is intrinsically...\n",
      "21  Mimetic Convergence aims to show the process o...\n",
      "22  Every content analysis should depart from a hy...\n",
      "23  As an evaluation approach, content analysis is...\n",
      "24  Qualitative content analysis is \"a systematic,...\n",
      "25  Holsti groups fifteen uses of content analysis...\n",
      "26  He also places these uses into the context of ...\n",
      "27  The following table shows fifteen uses of cont...\n",
      "28                                                   \n"
     ]
    }
   ],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    #strings starting with r are raw so their \\'s are not modifier characters\n",
    "    #If we didn't start with r the string would be: '\\\\[\\\\d+\\\\]'\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#convert to a DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `DataFrame` containing all relevant text from the page ready to be\n",
    "processed\n",
    "\n",
    "If you are not familiar with regex, it is a way of specifying searches in text.\n",
    "A regex engine takes in the search pattern, in the above case `'\\[\\d+\\]'` and\n",
    "some string, the paragraph texts. Then it reads the input string one character\n",
    "at a time checking if it matches the search. Here the regex `'\\d'` matches\n",
    "number characters (while `'\\['` and `'\\]'` capture the braces on either side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(36, 37), match='2'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python the regex package (`re`) usually returns `Match` objects (you can have\n",
    "multiple pattern hits in a a single `Match`), to get the string that matched our\n",
    "pattern we can use the `.group()` method, and as we want the first one we will\n",
    "ask for the 0'th group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the first number, if we wanted the whole block of numbers we can\n",
    "add a wildcard `'+'` which requests 1 or more instances of the preceding\n",
    "character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2134567890\n"
     ]
    }
   ],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the whole block of numbers, there are a huge number of special\n",
    "characters in regex, for the full description of Python's implementation look at\n",
    "the [re docs](https://docs.python.org/3/library/re.html) there is also a short\n",
    "[tutorial](https://docs.python.org/3/howto/regex.html#regex-howto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Section 1</span>\n",
    "<span style=\"color:red\">Construct cells immediately below this that describe and download webcontent relating to your anticipated final project. Use beautiful soup and at least five regular expressions to extract relevant, nontrivial *chunks* of that content (e.g., cleaned sentences, paragraphs, etc.) to a pandas `Dataframe`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped webnovel _Battle Through the Heavens_ from __[Wuxiaworld](http://www.wuxiaworld.com/btth-index/)__.<br><br>\n",
    "Wuxiaworld was founded by _RMX_, a lover of Wuxia, aka Chinese martial arts fictionin. It is currently the largest Chinese-to-English novel translation platform in the world. _Battle Through the Heavens_ is translated by a Chinese-American _goodguyperson_ and is serialized on Wuxiaworld. It is one of the most well-known Wuxia webnovels.<br><br>\n",
    "Though _Battle Through the Heavens_ is mainly published on Wuxiaworld, some of its early chapters are redirected to __[Hellotranslations](https://hellotranslations.wordpress.com/2015/01/30/dou-po-cang-qiong-chapter-1/)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 Genius No More\n",
      "‘Dou Zhi Li1, 3rd stage!’\n",
      "Facing the Magical Testing Monument as it displayed the 5 big hurtful words, the youth stood expressionless, lips curled in a small self-ridiculing smile. He tightly clenched his fist and because of the strength used, his slightly sharp fingernails dug deep into the palm of his hand, bringing brief moments of pain.\n",
      "“Xiao Yan, Dou Zhi Li, 3rd stage! Rank: Low!” Beside the Magical Testing Monument, a middle-aged man looked at the results on the monument and announced them with an indifferent voice.\n",
      "Immediately after the middle-aged man finished speaking, without much surprise, the people in the square started a commotion, ridiculing him.\n",
      "“3rd stage? Hmmhmm, as expected. This ‘genius’ has once again taken a step back.”\n",
      "“Ai, this piece of trash really disgraced his entire clan.”\n",
      "“If his father wasn’t the clan leader, this kind of trash would already have been kicked out of the clan. Nobody would care about him, and there wouldn’t be such a thing as leeching off the clan home.”\n",
      "“How could the once famous genius of Wu Tan Cheng2 fall to become like this over the past few years?”\n",
      "“Who knows? Maybe he did something unforgivable and caused the gods to get angry.”\n",
      "The ridicule and laughter directed at him came from all directions and resonated in the motionless youth’s ears, as if piercing his heart. He could not help but breathe heavily.\n",
      "He raised his head to reveal a delicate and immature face, jet black eyes glancing past the people of his age that ridiculed him as well. His lips, which were previously full of self-ridicule, turned to fill with bitterness instead.\n",
      "“These people, were they always this cold? Or was it because three years ago they smiled humbly to congratulate me, and now they wanted to take it back?” Smiling bitterly, Xiao Yan turned around and silently walked back to the group of people. His lonesome figure seemed to be unable to fit in with the surroundings.\n",
      "“Next up, Xiao Mei!”\n",
      "Hearing the tester’s voice, a teenage girl quickly ran up from in the crowd. The moment she got on stage, the murmurings and discussions quietened down and every pair of fiery eyes were locked on her face.\n",
      "The teenage girl wasn’t more than 14 years old. Although her beauty wasn’t immediately evident, her small childish face combined with her innocence captured the attention of the audience.\n",
      "She quickly stepped forward and put her tiny hands on the black stone monument. She then closed her eyes gently.\n",
      "As the girl closed her eyes, the monument shined brightly once again.\n",
      "‘Dou Zhi Li, 7th stage!’\n",
      "“Xiao Mei, Dou Zhi Li, 7th stage! Rank: High!”\n",
      "“Yeh!” Hearing the tester read out the result, the teen girl smiled proudly.\n",
      "“Tsk tsk, 7th stage Dou Zhi Li. How impressive! At this rate, in three years she’ll be named a true Dou Zhe3 already.”\n",
      "“She really fits the name of the clan’s seedling.”\n",
      "Hearing the unanimous praise coming from the crowd, the girl’s smile turned even wider. Vanity, the temptation that so many girls are unable to resist…\n",
      "While chatting with her fellow sister members, her line of sight weaved through the surrounding people and landed on a lonesome figure away from them.\n",
      "Furrowing her brows for a while, she finally decided against walking over. Between the two of them was already a huge gap. Looking at Xiao Yan’s performance these past few years, by the time the Adulthood Ceremony comes, he would only be able to place at the lower tier clan member ranks. She, however, with her brilliance, would become the clan’s very important and thus well-nurtured fighter. There would be no limit to her future.\n",
      "“Ai…” An inexplicable sigh emerged from her. Xiao Mei thought back to the youth from three years ago, bursting with energy and pride. At 4 he started practicing, and at 10 he achieved the 9th stage Dou Zhi Li. At 11 he broke the 10th stage Dou Zhi Li barrier and condensed his Dou Zhi Qi Zu4 successfully. He became the youngest Dou Zhe in the clan since the past 100 years.\n",
      "At that time, his self-confidence as well as immeasurable power attracted countless young teens which of course, included Xiao Mei.\n",
      "However, the path of a genius was always winding. Three years ago, this genius youngster whose fame reached the absolute top, received what could possibly be the cruelest blow. The hard work he put into acculmulating and condensing the Dou Zhi Qi Zu over the past ten years had, in just one night, vanished into nothingness. All the Dou Zhi Li in his body slowly dissipated with time and instead, pity for him grew.\n",
      "As a result of losing his Dou Zhi Li, his physical strength decreased as well.\n",
      "From the position of a genius, in one night he fell below the average person. This kind of blow made the youth lose his will to carry on training. The reverence once associated with this lad had slowly changed into disdain and ridicule.\n",
      "Standing so high up, and falling right down – this kind of fall might just be one that he could never recover from.\n",
      "“Next, Xiao Xun Er!”\n",
      "Among the commotion, the tester’s voice sounded yet again.\n",
      "Following the calling of this highly reputable name, the group of people quietened down immediately. Every single gaze turned.\n",
      "At the centerpoint of attention was a teen girl clad in a purple dress, elegantly standing there. Her calm, tender and immaculate face was completely unaffected by the many gazes of the crowd.\n",
      "Her calm and indifferent attitude could be compared to the blooming of a lotus. At such a young age, she already had the air of a refined lady. It would be hard to imagine how she would affect the city and the nation once she grew up.\n",
      "This purple dress girl, if compared to Xiao Mei in terms of beauty, was clearly several leagues above. It’s no wonder the crowd would have this kind of actions.\n",
      "Taking small, graceful steps, the girl named Xiao Xun Er walked up in front of the stone monument. She stretched her small hand out and the purple sleeve mixed with black and gold threads fell down her arm, revealing a delicate snow-white wrist. She touched the monument lightly.\n",
      "In the silence, the monument shone once more.\n",
      "“Dou Zhi Li, 9th stage! Rank: High!”\n",
      "Looking at the words on the monument, the entire square fell into deep silence.\n",
      "“…… She really reached the 9th stage, how frightening! The position of the youngest high rank in the clan has been taken, without a shred of doubt, by Miss Xun Er!” After the silence, several teens couldn’t help but wolf whistle, their eyes full of respect and awe.\n",
      "Dou Zhi Li was essential towards being a Dou Zhe. Dou Zhi Li is split into 10 different stages, and when the body acquires 10 stages of Dou Zhi Li, it can better condense the Dou Zhi Qi Zu, becoming a well-respected Dou Zhe!\n",
      "In the crowd of people, Xiao Mei stared at the purple dress girl in front of the monument with a little jealousy.\n",
      "Looking at the results on the monument, the middle-aged tester who would normally be indifferent smiled, faced her and congratulated her: “Miss Xun Er, half a year later, you should be able to condense the Dou Qi Zhi Zu5. If you succeed, you’ll be a Dou Zhe at the age of 14, the second person to do so in the Xiao clan.\n",
      "Of course, the second person. First would be the fallen genius, Xiao Yan.\n",
      "“Thanks.” The teen girl nodded her head lightly, her calm face showing a little happiness because of his praise. She quietly turned, and in the midst of the crowd’s attention, slowly walked to the downtrodden youth in the back of the group.\n",
      "“Brother Xiao Yan.” At the youth’s side, the teen girl stopped. She faced Xiao Yan and bowed respectfully. Her beautiful and gentle face showed a elegant smile which would make the surrounding girls jealous.\n",
      "“What qualifications do I have right now for you to call me that?” He faced the girl that could be considered the clan’s radiant pearl and said bitterly. She, after being down-hearted for an extremely short while, continued to maintain her respect.\n",
      "“Brother Xiao Yan, you once said to Xun Er6 before – to take on anything, one must first be able to let go. One is only truly free when he can take on and let go easily7.” Xiao Xun Er said gently, her smiling face full of warmth.\n",
      "“Haha, truly free? I only know how to say it. Look at me now, do I look like a free person? This world wasn’t mine to begin with.” Xiao Yan laughed at himself, saying dispiritedly.\n",
      "Facing Xiao Yan’s somber mood, Xiao Xun Er’s fine brows furrowed a little, and she said seriously: “Brother Xiao Yan, though I don’t know what’s happening to you, Xun Er honestly believes that you will stand again and reclaim your lost glory and respect…” She stopped for a moment, her white tender face reddening a little. “Back then, there were a lot of people who were attracted to you…”\n",
      "“Haha…” Hearing the girl’s whole-hearted truth, he laughed awkwardly but didn’t say anything else. Other people would be swayed by it, but he did not have the qualifications nor the mood. Instead, he silently turned around and walked away from the square.\n",
      "Standing still and facing the lonely back of the youth, she hesitated for a while before chasing after him and walking side-by-side with him. Meanwhile, from behind a jealous wolf whistle sounded(?).\n"
     ]
    }
   ],
   "source": [
    "# Scrape all texts of a chapter on hellotraclations\n",
    "hellotrans_request = requests.get('https://hellotranslations.wordpress.com/2015/01/30/dou-po-cang-qiong-chapter-1/')\n",
    "hellotrans_soup = bs4.BeautifulSoup(hellotrans_request.text, 'html.parser')\n",
    "hellotrans_ptags = hellotrans_soup.article.findAll('p')\n",
    "for h_ptag in hellotrans_ptags:\n",
    "    print(h_ptag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  chapter           title\n",
      "0       1  Genius No More\n"
     ]
    }
   ],
   "source": [
    "# Extract the chapter number and title on hellotraclations\n",
    "num_chapter = []\n",
    "titles = []\n",
    "\n",
    "## The first regex to extract the chapter number and title\n",
    "title = re.search(r'Chapter\\s(\\d+)\\s(.*)', hellotrans_ptags[0].text)\n",
    "num_chapter.append(title.group(1))\n",
    "titles.append(title.group(2))\n",
    "\n",
    "h_chapterDF = pandas.DataFrame({'chapter': num_chapter, 'title': titles})\n",
    "print(h_chapterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            paragraph\n",
      "0                           'Dou Zhi Li1, 3rd stage!'\n",
      "1   Facing the Magical Testing Monument as it disp...\n",
      "2   \"Xiao Yan, Dou Zhi Li, 3rd stage! Rank: Low!\" ...\n",
      "3   Immediately after the middle-aged man finished...\n",
      "4   \"3rd stage? Hmmhmm, as expected. This 'genius'...\n",
      "5   \"Ai, this piece of trash really disgraced his ...\n",
      "6   \"If his father wasn't the clan leader, this ki...\n",
      "7   \"How could the once famous genius of Wu Tan Ch...\n",
      "8   \"Who knows? Maybe he did something unforgivabl...\n",
      "9   The ridicule and laughter directed at him came...\n",
      "10  He raised his head to reveal a delicate and im...\n",
      "11  \"These people, were they always this cold? Or ...\n",
      "12                               \"Next up, Xiao Mei!\"\n",
      "13  Hearing the tester's voice, a teenage girl qui...\n",
      "14  The teenage girl wasn't more than 14 years old...\n",
      "15  She quickly stepped forward and put her tiny h...\n",
      "16  As the girl closed her eyes, the monument shin...\n",
      "17                           'Dou Zhi Li, 7th stage!'\n",
      "18     \"Xiao Mei, Dou Zhi Li, 7th stage! Rank: High!\"\n",
      "19  \"Yeh!\" Hearing the tester read out the result,...\n",
      "20  \"Tsk tsk, 7th stage Dou Zhi Li. How impressive...\n",
      "21  \"She really fits the name of the clan's seedli...\n",
      "22  Hearing the unanimous praise coming from the c...\n",
      "23  While chatting with her fellow sister members,...\n",
      "24  Furrowing her brows for a while, she finally d...\n",
      "25  \"Ai…\" An inexplicable sigh emerged from her. X...\n",
      "26  At that time, his self-confidence as well as i...\n",
      "27  However, the path of a genius was always windi...\n",
      "28  As a result of losing his Dou Zhi Li, his phys...\n",
      "29  From the position of a genius, in one night he...\n",
      "30  Standing so high up, and falling right down – ...\n",
      "31                               \"Next, Xiao Xun Er!\"\n",
      "32  Among the commotion, the tester's voice sounde...\n",
      "33  Following the calling of this highly reputable...\n",
      "34  At the centerpoint of attention was a teen gir...\n",
      "35  Her calm and indifferent attitude could be com...\n",
      "36  This purple dress girl, if compared to Xiao Me...\n",
      "37  Taking small, graceful steps, the girl named X...\n",
      "38      In the silence, the monument shone once more.\n",
      "39               \"Dou Zhi Li, 9th stage! Rank: High!\"\n",
      "40  Looking at the words on the monument, the enti...\n",
      "41  \"…… She really reached the 9th stage, how frig...\n",
      "42  Dou Zhi Li was essential towards being a Dou Z...\n",
      "43  In the crowd of people, Xiao Mei stared at the...\n",
      "44  Looking at the results on the monument, the mi...\n",
      "45  Of course, the second person. First would be t...\n",
      "46  \"Thanks.\" The teen girl nodded her head lightl...\n",
      "47  \"Brother Xiao Yan.\" At the youth's side, the t...\n",
      "48  \"What qualifications do I have right now for y...\n",
      "49  \"Brother Xiao Yan, you once said to Xun Er6 be...\n",
      "50  \"Haha, truly free? I only know how to say it. ...\n",
      "51  Facing Xiao Yan's somber mood, Xiao Xun Er's f...\n",
      "52  \"Haha…\" Hearing the girl's whole-hearted truth...\n",
      "53  Standing still and facing the lonely back of t...\n",
      "                                         conversation\n",
      "0       \"Xiao Yan, Dou Zhi Li, 3rd stage! Rank: Low!\"\n",
      "1   \"3rd stage? Hmmhmm, as expected. This 'genius'...\n",
      "2   \"Ai, this piece of trash really disgraced his ...\n",
      "3   \"If his father wasn't the clan leader, this ki...\n",
      "4   \"How could the once famous genius of Wu Tan Ch...\n",
      "5   \"Who knows? Maybe he did something unforgivabl...\n",
      "6   \"These people, were they always this cold? Or ...\n",
      "7                                \"Next up, Xiao Mei!\"\n",
      "8      \"Xiao Mei, Dou Zhi Li, 7th stage! Rank: High!\"\n",
      "9                                              \"Yeh!\"\n",
      "10  \"Tsk tsk, 7th stage Dou Zhi Li. How impressive...\n",
      "11  \"She really fits the name of the clan's seedli...\n",
      "12                                              \"Ai…\"\n",
      "13                               \"Next, Xiao Xun Er!\"\n",
      "14               \"Dou Zhi Li, 9th stage! Rank: High!\"\n",
      "15  \"…… She really reached the 9th stage, how frig...\n",
      "16                                          \"Thanks.\"\n",
      "17                                \"Brother Xiao Yan.\"\n",
      "18  \"What qualifications do I have right now for y...\n",
      "19  \"Brother Xiao Yan, you once said to Xun Er6 be...\n",
      "20  \"Haha, truly free? I only know how to say it. ...\n",
      "21  \"Brother Xiao Yan, though I don't know what's ...\n",
      "22                                            \"Haha…\"\n",
      "                                              sentence\n",
      "0                              Dou Zhi Li1, 3rd stage!\n",
      "1    Facing the Magical Testing Monument as it disp...\n",
      "2    He tightly clenched his fist and because of th...\n",
      "3                     Xiao Yan, Dou Zhi Li, 3rd stage!\n",
      "4                                           Rank: Low!\n",
      "5    Beside the Magical Testing Monument, a middle-...\n",
      "6    Immediately after the middle-aged man finished...\n",
      "7                                           3rd stage?\n",
      "8                                 Hmmhmm, as expected.\n",
      "9      This 'genius' has once again taken a step back.\n",
      "10   Ai, this piece of trash really disgraced his e...\n",
      "11   If his father wasn't the clan leader, this kin...\n",
      "12   Nobody would care about him, and there wouldn'...\n",
      "13   How could the once famous genius of Wu Tan Che...\n",
      "14                                          Who knows?\n",
      "15   Maybe he did something unforgivable and caused...\n",
      "16   The ridicule and laughter directed at him came...\n",
      "17              He could not help but breathe heavily.\n",
      "18   He raised his head to reveal a delicate and im...\n",
      "19   His lips, which were previously full of self-r...\n",
      "20           These people, were they always this cold?\n",
      "21   Or was it because three years ago they smiled ...\n",
      "22   Smiling bitterly, Xiao Yan turned around and s...\n",
      "23   His lonesome figure seemed to be unable to fit...\n",
      "24                                  Next up, Xiao Mei!\n",
      "25   Hearing the tester's voice, a teenage girl qui...\n",
      "26   The moment she got on stage, the murmurings an...\n",
      "27     The teenage girl wasn't more than 14 years old.\n",
      "28   Although her beauty wasn't immediately evident...\n",
      "29   She quickly stepped forward and put her tiny h...\n",
      "..                                                 ...\n",
      "87   In the crowd of people, Xiao Mei stared at the...\n",
      "88   Looking at the results on the monument, the mi...\n",
      "89   If you succeed, you'll be a Dou Zhe at the age...\n",
      "90                       Of course, the second person.\n",
      "91         First would be the fallen genius, Xiao Yan.\n",
      "92                                             Thanks.\n",
      "93   The teen girl nodded her head lightly, her cal...\n",
      "94   She quietly turned, and in the midst of the cr...\n",
      "95                                   Brother Xiao Yan.\n",
      "96         At the youth's side, the teen girl stopped.\n",
      "97          She faced Xiao Yan and bowed respectfully.\n",
      "98   Her beautiful and gentle face showed a elegant...\n",
      "99   What qualifications do I have right now for yo...\n",
      "100  He faced the girl that could be considered the...\n",
      "101  She, after being down-hearted for an extremely...\n",
      "102  Brother Xiao Yan, you once said to Xun Er6 bef...\n",
      "103  One is only truly free when he can take on and...\n",
      "104  Xiao Xun Er said gently, her smiling face full...\n",
      "105                                  Haha, truly free?\n",
      "106                         I only know how to say it.\n",
      "107      Look at me now, do I look like a free person?\n",
      "108              This world wasn't mine to begin with.\n",
      "109  Xiao Yan laughed at himself, saying dispiritedly.\n",
      "110  Facing Xiao Yan's somber mood, Xiao Xun Er's f...\n",
      "111  Back then, there were a lot of people who were...\n",
      "112  Haha…\" Hearing the girl's whole-hearted truth,...\n",
      "113  Other people would be swayed by it, but he did...\n",
      "114  Instead, he silently turned around and walked ...\n",
      "115  Standing still and facing the lonely back of t...\n",
      "116  Meanwhile, from behind a jealous wolf whistle ...\n",
      "\n",
      "[117 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the paragraphs, sentences, and conversations on hellotraclations\n",
    "paragraphs = []\n",
    "conversations = []\n",
    "sentences = []\n",
    "\n",
    "for para in hellotrans_ptags[1:]:\n",
    "    \n",
    "    ## The second regex to remove \"(?)\"\n",
    "    ## \"(?)\" indicates uncertainty during translation but does no good for our analysis\n",
    "    cleaned_para = re.sub(r'(\\(\\?\\))', '', para.text)\n",
    "    \n",
    "    ## The third regex to substitute all \"“\", \"”\", \"‘\", and \"’\" with \"\"\" and \"'\"\n",
    "    ## Due to some encoding issue, the text is using Chinese double quotation marks and single quotation marks\n",
    "    cleaned_para = re.sub(r'(\\“)', '\"', cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\”)', '\"', cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\‘)', \"'\", cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\’)', \"'\", cleaned_para)\n",
    "    \n",
    "    paragraphs.append(cleaned_para)\n",
    "    \n",
    "    ## The forth regex to extract conversations\n",
    "    conversation = re.findall(r'(\\\".*\\\")', cleaned_para)\n",
    "    for c in conversation:\n",
    "        cleaned_c = c.replace(u'\\xa0', u' ')\n",
    "        cleaned_c = cleaned_c.replace(u\"\\'\", u\"'\")\n",
    "        conversations.append(cleaned_c)\n",
    "\n",
    "    ## The fifth regex to split paragraphs into sentences\n",
    "    sentence = re.findall(r'([A-Z0-9][^\\.\\?\\!]*[\\.\\!\\?\\…]+)', cleaned_para)\n",
    "    for s in sentence:\n",
    "        cleaned_s = s.replace(u'\\xa0', u' ')\n",
    "        sentences.append(cleaned_s)\n",
    "\n",
    "h_paragraphDF = pandas.DataFrame({'paragraph': paragraphs})\n",
    "print(h_paragraphDF)\n",
    "h_conversationDF = pandas.DataFrame({'conversation': conversations})\n",
    "print(h_conversationDF)\n",
    "h_sentenceDF = pandas.DataFrame({'sentence': sentences})\n",
    "print(h_sentenceDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Chapter Next Chapter\n",
      "BTTH Chapter 12 – Stay Away from Him\n",
      "Looking at the delighted handsome youth, Xun Er’s slim eyebrows wrinkled. Not paying attention to his call, she turned away.\n",
      "“Miss Xun Er!”\n",
      "Looking at Xun Er from the side, the handsome but pale young man became immediately anxious and quickly crossed to block her from the front.\n",
      "Blocked by the youth, Xun Er stopped her steps. Her pair of long and beautiful eyes were lazily narrowed staring at him. Yet she did not utter a single word.\n",
      "“Miss Xun Er…” Stared at by pupils that were like water drops and despite the fact that he was used to flirting with beauties, his breathing became somewhat hurried. His clever tongue had lost all of its use for the moment.\n",
      "“Jia Lie Ao young master, if there isn’t anything urgent, please get out the way. I still have to do something.”\n",
      "Looking at this somewhat flush youth, Xun Er finally spoke. Her soft and young voice made the young man’s pale face flush with a sick red.\n",
      "“Hehe, Miss Xun Er, have you come to the market to buy something? I’m quite free right now so why don’t we look around the market together?” Taking a deep breath within his mind, Jia Lie Ao’s smile was open and gentle. This smile along with his status and handsomeness had successfully landed him quite a few girls.\n",
      "“Jia Lie Ao young master, I already said that I have something to do! Can you move aside?” Xun Er’s small mouth curves upward and her voice was smooth without any hint of irritation.\n",
      "Being rejected by Xun Er, the edges of Jia Lie Ao’s mouth twitched but he kept his smile and pulled out a bracelet from his pockets. The bracelet was a light blue gold color and was made from Blue Gold. From the bracelet hanged a smoothed green ball-like monster core. A soft green was emitted from the monster core and scattered its light on the bracelet, giving it a special hue. Looks like this intricate bracelet costs a lot!\n",
      "“Hehe, since Miss Xun Er has something to do then I, Jia Lie Ao won’t block you anymore.” Jiao Lei Ao tightly clutched the bracelet and smiles: “This was a bracelet that I just bought in the market, even though it’s not too expensive, it has a level one wood attribute monster core which greatly helps in the recovery of Dou Qi. Since Miss Xun Er hasn’t become a Dou Zhe yet, this bracelet is perfect for you. This is just a small gift of mine so please don’t say no. After all, I would lose face in front of my underlings…” At the end, Jia Lie Ao purposefully lowered his voice and the underlings around him, as if it was a play, grinned on cue.\n",
      "Looking at Jia Lie Ao’s actions, Xun Er’s brow lifted. She didn’t know how to deal with such a person.\n",
      "Right when she was about to refuse, her sight fell onto the green monster core on the bracelet and remembered how Xiao Yan was busily trying to find a wood attribute monster core. Her long eyelashes lightly blinked and her impassive face relaxed a bit…\n",
      "Looking at Xun Er’s relaxed face, Jia Lie Ao’s heart flutters happily and quickly pushes the Wood Attribute Bracelet forward: “Miss Xun Er, there’s no need to be courteous. Jia Lie Clan and Xiao Clan are both in the top three clans of Wu Tang City so exchanging little gifts is common.”\n",
      "“I’ll take the bracelet and remove the monster core and give to Xiao Yan ge-ge. As for the bracelet, when he’s not paying attention… I’ll throw it away.” With this mischievous thought, Xun Er didn’t hesitate anymore and stretches her hand out, about to take the bracelet. Suddenly a hand grabs her hand and stops her from taking the bracelet.\n",
      "Right when her hand was grabbed, Xun Er pauses in shock before having the Dou Qi in her body flowing in order to protect herself. But right when her hand was about to go free from the grasp, a young male hmph made her obediently stop struggling.\n",
      "Looking behind her, Xun Er saw Xiao Yan. When her sight moved a bit higher, she saw a harsh young face.\n",
      "“Don’t you know what he’s like?” Scowling at Xun Er, Xiao Yan criticized himself in his mind. Then he looks up and says: “Jia Lie Ao young master, your thought is graciously accepted by Xun Er but as for the gift, you should take it back.”\n",
      "Looking at the destroyed atmosphere, a hint of anger flashed within Jia Lie Ao’s eyes. But, in front of Xun Er, he tried to keep his “gentlemanly” air and waxily smiles: “Xiao Yan young master, I saw that Miss Xun Er didn’t have any jewelry so I wanted to help her a bit. Do you not want to let her have a few small trinkets to accentuate her beauty?”\n",
      "Helplessly sighing, Xiao Yan glances at the Wood Attribute bracelet in Jia Lie Ao’s hand and took out another green bracelet and with some frustration, asks: “Do you really like bracelets? Here you go, don’t take other people’s stuff for no reason. I already told you that there is no such thing as a free lunch. The ones who offer free stuff always have a hidden motive. With your innocent look, you may have been sold by someone and still wouldn’t know what had happened.”\n",
      "Hearing Xiao Yan’s words that were obviously directed at him, Jia Lie Ao’s face becomes cold. But when he saw the bracelet on Xiao Yan’s hand, he couldn’t help but laugh.\n",
      "The bracelet in Xiao Yan’s hands, from a material standpoint, couldn’t have costed more that 5 Gold Coins. While, his Wood Attribute Bracelet, which had an authentic monster core, cost an entire 1000+ gold coins. The two bracelets, no matter how you look at it, either price or actual usefulness, had a huge difference and Xiao Yan’s bracelet couldn’t even compare to the Wood Attribute Bracelet. So, when Jia Lie Ao saw Xiao Yan giving such a poor bracelet to the beautiful Xun Er, he couldn’t help but criticize Xiao Yan: “Xiao Yan, I know that you don’t have a high position in your clan, but… but why would you give such pitiful thing to Xun Er?”\n",
      "Ignoring Jia Lie Ao’s taunt, Xiao Yan looked at the young girl who was staring at the bracelet in his hand and hurriedly asked: “Do you want it or not? If you don’t then I’ll just throw it away, it was only 2-3 gold coins.”\n",
      "“Haa….” Hearing Xiao Yan’s words, not only did Jia Lie Ao start laughing, his underlings also started to laugh at Xiao Yan with a ridiculing tone.\n",
      "But the ridiculing laugh didn’t continue for long before being cut off as if they had just had their necks cut off. On everyone was a hilariously stunned face.\n",
      "Xun Er who had been stunned, responded quickly to Xiao Yan’s words. Her two hands almost instinctively reached out and snatched the bracelet in Xiao Yan’s hand. After getting the bracelet, Xun Er realized what she had done, perhaps she had acted a bit too impatient…\n",
      "A light red blush appeared on her delicate face but Xun Er wasn’t like other people and after a slight period of embarrassment, she graciously hooked the bracelet onto her white wrist. Raising her head and giving a coy smile, she said: “Thank you Xiao Yan ge-ge.”\n",
      "With an ugly face, Jia Lie Ao stared at Xun Er who was quite intimate with Xiao Yan. On his face was apparent jealousy and he said: “Hehe, I didn’t realize that Miss Xun Er’s preferences were so unique. I guess I’ve been mistaken.”\n",
      "Xiao Yan glances at Jia Lie Ao in front of him and his gaze landed on the gold star at his chest. He strangely thought: When I saw him last year, he was only 9 Duan Qi right? Who would have thought that he would be successful in compressing his Dou Qi Cyclone. But, to become a Dou Zhe at the age of 21, his talent is barely decent…\n",
      "Seeing that Jia Lie Ao had no intention of leaving, Xiao Yan pursed his lips. His wasn’t affected by the power and status behind Jia Lie Ao and since the Xiao Clan and Jia Lie Clan didn’t have good relations in the first place, there was no need for him to act humble. Stroking his nose, Xiao Yan lightly said: “Jia Lia Ao young master, your womanizer habits are known by the entire Wu Tang City. Xun Er is still young and doesn’t have time to play with your flirting games so hopefully you can go after other girls in the future.”\n",
      "“Stay away from him!”\n",
      "After speaking to Jia Lie Ao, Xiao Yan ignored the green-faced Jia Lie Ao and used his age as an advantage to arrogantly speak to Xun Er.\n",
      "“Okay.”\n",
      "Xun Er’s agile eyes blinked and nodded without any hesitation. To her, Jia Lie Ao was just a stranger that she saw a couple of times while Xiao Yan, for her, was irreplaceable. Since Xiao Yan told her to stay away from Jia Lie Ao, she’ll just stay away from him.\n",
      "The choice wasn’t a hard one for Xun Er.\n"
     ]
    }
   ],
   "source": [
    "# Scrape all texts of a chapter on wuxiaworld\n",
    "wuxia_request = requests.get('http://www.wuxiaworld.com/btth-index/btth-chapter-12/')\n",
    "wuxia_soup = bs4.BeautifulSoup(wuxia_request.text, 'html.parser')\n",
    "wuxia_ptags = wuxia_soup.find(itemprop=\"articleBody\").findAll('p')\n",
    "for w_ptag in wuxia_ptags:\n",
    "    print(w_ptag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  chapter               title\n",
      "0      12  Stay Away from Him\n"
     ]
    }
   ],
   "source": [
    "# Extract the chapter number and title on wuxiaworld\n",
    "num_chapter = []\n",
    "titles = []\n",
    "\n",
    "title = re.search(r'BTTH\\sChapter\\s(\\d+)\\s–\\s(.*)', wuxia_ptags[1].text)\n",
    "\n",
    "num_chapter.append(title.group(1))\n",
    "titles.append(title.group(2))\n",
    "\n",
    "w_chapterDF = pandas.DataFrame({'chapter': num_chapter, 'title': titles})\n",
    "print(w_chapterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            paragraph\n",
      "0   Looking at the delighted handsome youth, Xun E...\n",
      "1                                      \"Miss Xun Er!\"\n",
      "2   Looking at Xun Er from the side, the handsome ...\n",
      "3   Blocked by the youth, Xun Er stopped her steps...\n",
      "4   \"Miss Xun Er…\" Stared at by pupils that were l...\n",
      "5   \"Jia Lie Ao young master, if there isn't anyth...\n",
      "6   Looking at this somewhat flush youth, Xun Er f...\n",
      "7   \"Hehe, Miss Xun Er, have you come to the marke...\n",
      "8   \"Jia Lie Ao young master, I already said that ...\n",
      "9   Being rejected by Xun Er, the edges of Jia Lie...\n",
      "10  \"Hehe, since Miss Xun Er has something to do t...\n",
      "11  Looking at Jia Lie Ao's actions, Xun Er's brow...\n",
      "12  Right when she was about to refuse, her sight ...\n",
      "13  Looking at Xun Er's relaxed face, Jia Lie Ao's...\n",
      "14  \"I'll take the bracelet and remove the monster...\n",
      "15  Right when her hand was grabbed, Xun Er pauses...\n",
      "16  Looking behind her, Xun Er saw Xiao Yan. When ...\n",
      "17  \"Don't you know what he's like?\" Scowling at X...\n",
      "18  Looking at the destroyed atmosphere, a hint of...\n",
      "19  Helplessly sighing, Xiao Yan glances at the Wo...\n",
      "20  Hearing Xiao Yan's words that were obviously d...\n",
      "21  The bracelet in Xiao Yan's hands, from a mater...\n",
      "22  Ignoring Jia Lie Ao's taunt, Xiao Yan looked a...\n",
      "23  \"Haa….\" Hearing Xiao Yan's words, not only did...\n",
      "24  But the ridiculing laugh didn't continue for l...\n",
      "25  Xun Er who had been stunned, responded quickly...\n",
      "26  A light red blush appeared on her delicate fac...\n",
      "27  With an ugly face, Jia Lie Ao stared at Xun Er...\n",
      "28  Xiao Yan glances at Jia Lie Ao in front of him...\n",
      "29  Seeing that Jia Lie Ao had no intention of lea...\n",
      "30                              \"Stay away from him!\"\n",
      "31  After speaking to Jia Lie Ao, Xiao Yan ignored...\n",
      "32                                            \"Okay.\"\n",
      "33  Xun Er's agile eyes blinked and nodded without...\n",
      "34           The choice wasn't a hard one for Xun Er.\n",
      "                                         conversation\n",
      "0                                      \"Miss Xun Er!\"\n",
      "1                                      \"Miss Xun Er…\"\n",
      "2   \"Jia Lie Ao young master, if there isn't anyth...\n",
      "3   \"Hehe, Miss Xun Er, have you come to the marke...\n",
      "4   \"Jia Lie Ao young master, I already said that ...\n",
      "5   \"Hehe, since Miss Xun Er has something to do t...\n",
      "6   \"Miss Xun Er, there's no need to be courteous....\n",
      "7   \"I'll take the bracelet and remove the monster...\n",
      "8   \"Don't you know what he's like?\" Scowling at X...\n",
      "9   \"gentlemanly\" air and waxily smiles: \"Xiao Yan...\n",
      "10  \"Do you really like bracelets? Here you go, do...\n",
      "11  \"Xiao Yan, I know that you don't have a high p...\n",
      "12  \"Do you want it or not? If you don't then I'll...\n",
      "13                                            \"Haa….\"\n",
      "14                        \"Thank you Xiao Yan ge-ge.\"\n",
      "15  \"Hehe, I didn't realize that Miss Xun Er's pre...\n",
      "16  \"Jia Lia Ao young master, your womanizer habit...\n",
      "17                              \"Stay away from him!\"\n",
      "18                                            \"Okay.\"\n",
      "                                             sentence\n",
      "0   Looking at the delighted handsome youth, Xun E...\n",
      "1   Not paying attention to his call, she turned a...\n",
      "2                                        Miss Xun Er!\n",
      "3   Looking at Xun Er from the side, the handsome ...\n",
      "4     Blocked by the youth, Xun Er stopped her steps.\n",
      "5   Her pair of long and beautiful eyes were lazil...\n",
      "6                Yet she did not utter a single word.\n",
      "7   Miss Xun Er…\" Stared at by pupils that were li...\n",
      "8   His clever tongue had lost all of its use for ...\n",
      "9   Jia Lie Ao young master, if there isn't anythi...\n",
      "10                      I still have to do something.\n",
      "11  Looking at this somewhat flush youth, Xun Er f...\n",
      "12  Her soft and young voice made the young man's ...\n",
      "13  Hehe, Miss Xun Er, have you come to the market...\n",
      "14  I'm quite free right now so why don't we look ...\n",
      "15  Taking a deep breath within his mind, Jia Lie ...\n",
      "16  This smile along with his status and handsomen...\n",
      "17  Jia Lie Ao young master, I already said that I...\n",
      "18                                Can you move aside?\n",
      "19  Xun Er's small mouth curves upward and her voi...\n",
      "20  Being rejected by Xun Er, the edges of Jia Lie...\n",
      "21  The bracelet was a light blue gold color and w...\n",
      "22  From the bracelet hanged a smoothed green ball...\n",
      "23  A soft green was emitted from the monster core...\n",
      "24    Looks like this intricate bracelet costs a lot!\n",
      "25  Hehe, since Miss Xun Er has something to do th...\n",
      "26  Jiao Lei Ao tightly clutched the bracelet and ...\n",
      "27  Since Miss Xun Er hasn't become a Dou Zhe yet,...\n",
      "28  This is just a small gift of mine so please do...\n",
      "29  After all, I would lose face in front of my un...\n",
      "..                                                ...\n",
      "60  So, when Jia Lie Ao saw Xiao Yan giving such a...\n",
      "61  Ignoring Jia Lie Ao's taunt, Xiao Yan looked a...\n",
      "62  If you don't then I'll just throw it away, it ...\n",
      "63                                              Haa….\n",
      "64  Hearing Xiao Yan's words, not only did Jia Lie...\n",
      "65  But the ridiculing laugh didn't continue for l...\n",
      "66        On everyone was a hilariously stunned face.\n",
      "67  Xun Er who had been stunned, responded quickly...\n",
      "68  Her two hands almost instinctively reached out...\n",
      "69  After getting the bracelet, Xun Er realized wh...\n",
      "70  A light red blush appeared on her delicate fac...\n",
      "71  Raising her head and giving a coy smile, she s...\n",
      "72  With an ugly face, Jia Lie Ao stared at Xun Er...\n",
      "73  On his face was apparent jealousy and he said:...\n",
      "74                        I guess I've been mistaken.\n",
      "75  Xiao Yan glances at Jia Lie Ao in front of him...\n",
      "76  He strangely thought: When I saw him last year...\n",
      "77  Who would have thought that he would be succes...\n",
      "78  But, to become a Dou Zhe at the age of 21, his...\n",
      "79  Seeing that Jia Lie Ao had no intention of lea...\n",
      "80  His wasn't affected by the power and status be...\n",
      "81  Stroking his nose, Xiao Yan lightly said: \"Jia...\n",
      "82  Xun Er is still young and doesn't have time to...\n",
      "83                                Stay away from him!\n",
      "84  After speaking to Jia Lie Ao, Xiao Yan ignored...\n",
      "85                                              Okay.\n",
      "86  Xun Er's agile eyes blinked and nodded without...\n",
      "87  To her, Jia Lie Ao was just a stranger that sh...\n",
      "88  Since Xiao Yan told her to stay away from Jia ...\n",
      "89           The choice wasn't a hard one for Xun Er.\n",
      "\n",
      "[90 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the paragraphs, sentences, conversations on wuxiaworld\n",
    "paragraphs = []\n",
    "conversations = []\n",
    "sentences = []\n",
    "\n",
    "for para in wuxia_ptags[2:]:\n",
    "    \n",
    "    cleaned_para = re.sub(r'(\\(\\?\\))', '', para.text)\n",
    "    \n",
    "    cleaned_para = re.sub(r'(\\“)', '\"', cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\”)', '\"', cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\‘)', \"'\", cleaned_para)\n",
    "    cleaned_para = re.sub(r'(\\’)', \"'\", cleaned_para)\n",
    "    \n",
    "    paragraphs.append(cleaned_para)\n",
    "\n",
    "    conversation = re.findall(r'(\\\".*\\\")', cleaned_para)\n",
    "    for c in conversation:\n",
    "        cleaned_c = c.replace(u'\\xa0', u' ')\n",
    "        cleaned_c = cleaned_c.replace(u\"\\'\", u\"'\")\n",
    "        conversations.append(cleaned_c)\n",
    "\n",
    "    sentence = re.findall(r'([A-Z0-9][^\\.\\?\\!]*[\\.\\!\\?\\…]+)', cleaned_para)\n",
    "    for s in sentence:\n",
    "        cleaned_s = s.replace(u'\\xa0', u' ')\n",
    "        sentences.append(cleaned_s)\n",
    "\n",
    "w_paragraphDF = pandas.DataFrame({'paragraph': paragraphs})\n",
    "print(w_paragraphDF)\n",
    "w_conversationDF = pandas.DataFrame({'conversation': conversations})\n",
    "print(w_conversationDF)\n",
    "w_sentenceDF = pandas.DataFrame({'sentence': sentences})\n",
    "print(w_sentenceDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_soup(url):\n",
    "    btth_request = requests.get(url)\n",
    "    btth_soup = bs4.BeautifulSoup(btth_request.text, 'html.parser')\n",
    "    return btth_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     chapter                                              title\n",
      "0          1                                     Genius No More\n",
      "1          2                                   Dou Qi Continent\n",
      "2          3                                             Guests\n",
      "3          4                        Faction of the Misty Clouds\n",
      "4          5                                Qi Gathering Powder\n",
      "5          6                                         Alchemists\n",
      "6          7                                           Divorce!\n",
      "7          8                               The Mysterious Elder\n",
      "8          9                                            Yao Lao\n",
      "9         10                                    Borrowing Money\n",
      "10        11                                         The Market\n",
      "11        12                                 Stay Away From Him\n",
      "12        13                                  Black Metal Piece\n",
      "13        14                                        Vacuum Hand\n",
      "14        15                                           Training\n",
      "15        16                                          Xiao Ning\n",
      "16        17                                           Conflict\n",
      "17        18        Xuan High Level Dou Technique: Octane Blast\n",
      "18        19                                     Cruel Training\n",
      "19        20                                            Auction\n",
      "20        21                        Second Tier Alchemist Gu Ni\n",
      "21        22                                    Hurricane Chant\n",
      "22        23                                       The Scramble\n",
      "23        24                               Everything For Later\n",
      "24        25                                    I’ll Pay For It\n",
      "25        26                                 Intensive Training\n",
      "26        27                                  Rushing 7 Duan Qi\n",
      "27        28                          Strengthening Vacuum Hand\n",
      "28        29                                   An Important Day\n",
      "29        30   The one who humiliates gets humiliated in the...\n",
      "...      ...                                                ...\n",
      "1489    1517                     Purifying Demonic Lotus Saint?\n",
      "1490    1518                         Demon Saint VS Demon Flame\n",
      "1491    1519                                          Stripping\n",
      "1492    1520                                       Final Reward\n",
      "1493    1521                          Refining the Demon Flame!\n",
      "1494    1522                       Change In the Central Plains\n",
      "1495    1523                         Breaking Out of the Cocoon\n",
      "1496    1524                                          Fire Baby\n",
      "1497    1525                                 Demon Flame Plains\n",
      "1498    1526                    Destroy With The Flip Of A Hand\n",
      "1499    1527                                     Imminent Storm\n",
      "1500    1528                                     Returning Home\n",
      "1501    1529                                  Challenge Letter!\n",
      "1502    1530                                    Fallen Mountain\n",
      "1503    1531                                        Hun Qian Mo\n",
      "1504    1533                                               Draw\n",
      "1505    1535                     Xiao Yan VS Hall of Soul Chief\n",
      "1506    1536                           Nihility Devouring Flame\n",
      "1507    1541                                            Message\n",
      "1508    1542                                         Huang Tian\n",
      "1509    1543                   Confrontation Between Two Tribes\n",
      "1510    1544                                     Exchange Blows\n",
      "1511    1545                         Nine Coloured Light Pillar\n",
      "1512    1546                        Cai Lin Exiting Her Retreat\n",
      "1513    1547                                  Unexpected Change\n",
      "1514    1548                Transforming Dragon Demon Formation\n",
      "1515    1549                             Extermination Fire Bod\n",
      "1516    1550                         Crazy Northern Dragon King\n",
      "1517    1551                                               Kill\n",
      "1518    1552                                  Refining A Puppet\n",
      "\n",
      "[1519 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# After a closer look at different chapters, I found that the chapter number and title are structured quite differently\n",
    "# across pages, thus I scraped the whole list of chapter titles from the index page\n",
    "# Note that some chapters are not listed on index page and do not obtain titles\n",
    "index_soup = prepare_soup('http://www.wuxiaworld.com/btth-index/')\n",
    "num_chapters = []\n",
    "titles = []\n",
    "\n",
    "# Chapter 1 - 1499\n",
    "div_tags = index_soup.find(itemprop = 'articleBody').findAll('div')\n",
    "for ptags in div_tags:\n",
    "    title_list = re.findall(r'Chapter\\s(\\d+)\\:(.*)', ptags.text)\n",
    "    for (num_chap, title) in title_list:\n",
    "        num_chapters.append(num_chap)\n",
    "        titles.append(title)\n",
    "\n",
    "# Chapter 1500 - 1552\n",
    "last_53 = index_soup.find(itemprop = 'articleBody').findAll('p')[-1]\n",
    "title_list = re.findall(r'Chapter\\s(\\d+)\\:(.*)', last_53.text)\n",
    "for (num_chap, title) in title_list:\n",
    "    num_chapters.append(num_chap)\n",
    "    titles.append(title)\n",
    "\n",
    "# Construct the chapterDF dataframe\n",
    "chapterDF = pandas.DataFrame({'chapter': num_chapters, 'title': titles})\n",
    "print(chapterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wrap up the above code into a function that reads a url and the corresponding soup \n",
    "# and consructs three dataframes including paragraghs, conversations, and sentences\n",
    "def btth_scraper(url, btth_soup):\n",
    "    paragraphs = []\n",
    "    conversations = []\n",
    "    sentences = []\n",
    "\n",
    "    if url[8:25] == 'hellotranslations':\n",
    "        btth_ptags = btth_soup.article.findAll('p')\n",
    "        start_row = 1\n",
    "    elif url[11:21] == 'wuxiaworld':\n",
    "        btth_ptags = btth_soup.find(itemprop=\"articleBody\").findAll('p')\n",
    "        if btth_ptags[1].text == 'Battle Through the Heavens':\n",
    "            start_row = 2\n",
    "        else:\n",
    "            start_row = 1\n",
    "\n",
    "    for para in btth_ptags[start_row:]:\n",
    "        cleaned_para = re.sub(r'(\\(\\?\\))', '', para.text)\n",
    "        cleaned_para = re.sub(r'(\\“)', '\"', cleaned_para)\n",
    "        cleaned_para = re.sub(r'(\\”)', '\"', cleaned_para)\n",
    "        cleaned_para = re.sub(r'(\\‘)', \"'\", cleaned_para)\n",
    "        cleaned_para = re.sub(r'(\\’)', \"'\", cleaned_para)\n",
    "        paragraphs.append(cleaned_para)\n",
    "\n",
    "        conversation = re.findall(r'(\\\".*\\\")', cleaned_para)\n",
    "        for c in conversation:\n",
    "            cleaned_c = c.replace(u'\\xa0', u' ')\n",
    "            cleaned_c = cleaned_c.replace(u\"\\'\", u\"'\")\n",
    "            conversations.append(cleaned_c)\n",
    "\n",
    "        sentence = re.findall(r'([A-Z0-9][^\\.\\?\\!]*[\\.\\!\\?\\…]+)', cleaned_para)\n",
    "        for s in sentence:\n",
    "            cleaned_s = s.replace(u'\\xa0', u' ')\n",
    "            sentences.append(cleaned_s)\n",
    "\n",
    "    paragraphDF = pandas.DataFrame({'paragraph': paragraphs})\n",
    "    conversationDF = pandas.DataFrame({'conversation': conversations})\n",
    "    sentenceDF = pandas.DataFrame({'sentence': sentences})\n",
    "    \n",
    "    return paragraphDF, conversationDF, sentenceDF  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spidering\n",
    "\n",
    "What if we want to to get a bunch of different pages from wikipedia. We would\n",
    "need to get the url for each of the pages we want. Typically, we want pages that\n",
    "are linked to by other pages and so we will need to parse pages and identify the\n",
    "links. Right now we will be retrieving all links in the body of the content\n",
    "analysis page.\n",
    "\n",
    "To do this we will need to find all the `<a>` (anchor) tags with `href`s\n",
    "(hyperlink references) inside of `<p>` tags. `href` can have many\n",
    "[different](http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-\n",
    "it-used) [forms](https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML) so\n",
    "dealing with them can be tricky, but generally, you will want to extract\n",
    "absolute or relative links. An absolute link is one you can follow without\n",
    "modification, while a relative link requires a base url that you will then\n",
    "append. Wikipedia uses relative urls for its internal links: below is an example\n",
    "for dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://en.wikipedia.org/wiki/Document', 1, 'documents'), ('https://en.wikipedia.org/wiki/Text_(literary_theory)', 2, 'texts'), ('https://en.wikipedia.org/wiki/Semantics', 2, 'meaningful'), ('https://en.wikipedia.org/wiki/Machine_learning', 3, 'Machine learning'), ('https://en.wikipedia.org/wiki/Klaus_Krippendorff', 6, 'Klaus Krippendorff'), ('https://en.wikipedia.org/wiki/Radio', 7, 'radio'), ('https://en.wikipedia.org/wiki/Television', 7, 'television'), ('https://en.wikipedia.org/wiki/Key_Word_in_Context', 7, 'Keyword In Context'), ('https://en.wikipedia.org/wiki/Synonym', 7, 'synonyms'), ('https://en.wikipedia.org/wiki/Homonym', 7, 'homonyms')]\n"
     ]
    }
   ],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#We also want to know where the links come from so we also will get:\n",
    "#the paragraph number\n",
    "#the word the link is in\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    #we only want hrefs that link to wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #We need to extract the url from the <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #wikipedia_base_url is the base we can use the urllib joining function to merge them\n",
    "        #Giving a nice structured tupe like this means we can use tuple expansion later\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be adding these new texts to our DataFrame `contentParagraphsDF` so we\n",
    "will need to add 2 more columns to keep track of paragraph numbers and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>paragraph-number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nContent analysis is a research method for st...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Practices and philosophies of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computers are increasingly used in content ana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Content analysis is best understood as a broad...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The simplest and most objective form of conten...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>More generally, content analysis is research u...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>By having contents of communication available ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Robert Weber notes: \"To make valid inferences ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>There are five types of texts in content analy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Recently, Arash Heydarian Pashakhanlou has arg...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mimetic Convergence aims to show the process o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Qualitative content analysis is \"a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paragraph-text  \\\n",
       "0                                                       \n",
       "1   \\nContent analysis is a research method for st...   \n",
       "2   Practices and philosophies of content analysis...   \n",
       "3   Computers are increasingly used in content ana...   \n",
       "4                                                       \n",
       "5                                                       \n",
       "6   Content analysis is best understood as a broad...   \n",
       "7   The simplest and most objective form of conten...   \n",
       "8   A further step in analysis is the distinction ...   \n",
       "9   More generally, content analysis is research u...   \n",
       "10  By having contents of communication available ...   \n",
       "11  Robert Weber notes: \"To make valid inferences ...   \n",
       "12  There are five types of texts in content analy...   \n",
       "13  Over the years, content analysis has been appl...   \n",
       "14  In recent times, particularly with the advent ...   \n",
       "15  Quantitative content analysis has enjoyed a re...   \n",
       "16  Recently, Arash Heydarian Pashakhanlou has arg...   \n",
       "17  Content analysis can also be described as stud...   \n",
       "18  The method of content analysis enables the res...   \n",
       "19  Since the 1980s, content analysis has become a...   \n",
       "20  The creation of coding frames is intrinsically...   \n",
       "21  Mimetic Convergence aims to show the process o...   \n",
       "22  Every content analysis should depart from a hy...   \n",
       "23  As an evaluation approach, content analysis is...   \n",
       "24  Qualitative content analysis is \"a systematic,...   \n",
       "25  Holsti groups fifteen uses of content analysis...   \n",
       "26  He also places these uses into the context of ...   \n",
       "27  The following table shows fifteen uses of cont...   \n",
       "28                                                      \n",
       "\n",
       "                                            source  paragraph-number  \n",
       "0   https://en.wikipedia.org/wiki/Content_analysis                 0  \n",
       "1   https://en.wikipedia.org/wiki/Content_analysis                 1  \n",
       "2   https://en.wikipedia.org/wiki/Content_analysis                 2  \n",
       "3   https://en.wikipedia.org/wiki/Content_analysis                 3  \n",
       "4   https://en.wikipedia.org/wiki/Content_analysis                 4  \n",
       "5   https://en.wikipedia.org/wiki/Content_analysis                 5  \n",
       "6   https://en.wikipedia.org/wiki/Content_analysis                 6  \n",
       "7   https://en.wikipedia.org/wiki/Content_analysis                 7  \n",
       "8   https://en.wikipedia.org/wiki/Content_analysis                 8  \n",
       "9   https://en.wikipedia.org/wiki/Content_analysis                 9  \n",
       "10  https://en.wikipedia.org/wiki/Content_analysis                10  \n",
       "11  https://en.wikipedia.org/wiki/Content_analysis                11  \n",
       "12  https://en.wikipedia.org/wiki/Content_analysis                12  \n",
       "13  https://en.wikipedia.org/wiki/Content_analysis                13  \n",
       "14  https://en.wikipedia.org/wiki/Content_analysis                14  \n",
       "15  https://en.wikipedia.org/wiki/Content_analysis                15  \n",
       "16  https://en.wikipedia.org/wiki/Content_analysis                16  \n",
       "17  https://en.wikipedia.org/wiki/Content_analysis                17  \n",
       "18  https://en.wikipedia.org/wiki/Content_analysis                18  \n",
       "19  https://en.wikipedia.org/wiki/Content_analysis                19  \n",
       "20  https://en.wikipedia.org/wiki/Content_analysis                20  \n",
       "21  https://en.wikipedia.org/wiki/Content_analysis                21  \n",
       "22  https://en.wikipedia.org/wiki/Content_analysis                22  \n",
       "23  https://en.wikipedia.org/wiki/Content_analysis                23  \n",
       "24  https://en.wikipedia.org/wiki/Content_analysis                24  \n",
       "25  https://en.wikipedia.org/wiki/Content_analysis                25  \n",
       "26  https://en.wikipedia.org/wiki/Content_analysis                26  \n",
       "27  https://en.wikipedia.org/wiki/Content_analysis                27  \n",
       "28  https://en.wikipedia.org/wiki/Content_analysis                28  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can add two more columns to our `Dataframe` and define a function to\n",
    "parse\n",
    "each linked page and add its text to our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Make a dict to store data before adding it to the DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Now we get the page\n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #enumerating gives use the paragraph number\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #same regex as before\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run it on our list of link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph-number</th>\n",
       "      <th>paragraph-text</th>\n",
       "      <th>source</th>\n",
       "      <th>source-paragraph-number</th>\n",
       "      <th>source-paragraph-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nContent analysis is a research method for st...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Practices and philosophies of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Computers are increasingly used in content ana...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Content analysis is best understood as a broad...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>The simplest and most objective form of conten...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A further step in analysis is the distinction ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>More generally, content analysis is research u...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>By having contents of communication available ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Robert Weber notes: \"To make valid inferences ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>There are five types of texts in content analy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Over the years, content analysis has been appl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>In recent times, particularly with the advent ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Quantitative content analysis has enjoyed a re...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Recently, Arash Heydarian Pashakhanlou has arg...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Content analysis can also be described as stud...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>The method of content analysis enables the res...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Since the 1980s, content analysis has become a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>The creation of coding frames is intrinsically...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Mimetic Convergence aims to show the process o...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Every content analysis should depart from a hy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>As an evaluation approach, content analysis is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Qualitative content analysis is \"a systematic,...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Holsti groups fifteen uses of content analysis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>He also places these uses into the context of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>The following table shows fifteen uses of cont...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Content_analysis</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>A document is a written, drawn, presented, or ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Document</td>\n",
       "      <td>1</td>\n",
       "      <td>documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6</td>\n",
       "      <td>Relying on literary theory, the notion of text...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Text_(literary_t...</td>\n",
       "      <td>2</td>\n",
       "      <td>texts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>Semantics (from Ancient Greek: σημαντικός sēma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>In international scientific vocabulary semanti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>The formal study of semantics intersects with ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>Semantics contrasts with syntax, the study of ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>6</td>\n",
       "      <td>In linguistics, semantics is the subfield that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>7</td>\n",
       "      <td>In the late 1960s, Richard Montague proposed a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8</td>\n",
       "      <td>Despite its elegance, Montague grammar was lim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>9</td>\n",
       "      <td>In Chomskyan linguistics there was no mechanis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>This view of semantics, as an innate finite me...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>11</td>\n",
       "      <td>A concrete example of the latter phenomenon is...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>12</td>\n",
       "      <td>Each of a set of synonyms like redouter ('to d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>13</td>\n",
       "      <td>and may go back to earlier Indian views on lan...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>14</td>\n",
       "      <td>An attempt to defend a system based on proposi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>15</td>\n",
       "      <td>Another set of concepts related to fuzziness i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>16</td>\n",
       "      <td>Systems of categories are not objectively out ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>17</td>\n",
       "      <td>Originates from Montague's work (see above). A...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>18</td>\n",
       "      <td>Pioneered by the philosopher Donald Davidson, ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>19</td>\n",
       "      <td>This theory is an effort to explain properties...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20</td>\n",
       "      <td>A linguistic theory that investigates word mea...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>21</td>\n",
       "      <td>Computational semantics is focused on the proc...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>22</td>\n",
       "      <td>In computer science, the term semantics refers...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>23</td>\n",
       "      <td>The semantics of programming languages and oth...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>24</td>\n",
       "      <td>For instance, the following statements use dif...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>25</td>\n",
       "      <td>Various ways have been developed to describe t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>26</td>\n",
       "      <td>The Semantic Web refers to the extension of th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>27</td>\n",
       "      <td>In psychology, semantic memory is memory for m...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>28</td>\n",
       "      <td>Ideasthesia is a psychological phenomenon in w...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Semantics</td>\n",
       "      <td>2</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    paragraph-number                                     paragraph-text  \\\n",
       "0                  0                                                      \n",
       "1                  1  \\nContent analysis is a research method for st...   \n",
       "2                  2  Practices and philosophies of content analysis...   \n",
       "3                  3  Computers are increasingly used in content ana...   \n",
       "4                  4                                                      \n",
       "5                  5                                                      \n",
       "6                  6  Content analysis is best understood as a broad...   \n",
       "7                  7  The simplest and most objective form of conten...   \n",
       "8                  8  A further step in analysis is the distinction ...   \n",
       "9                  9  More generally, content analysis is research u...   \n",
       "10                10  By having contents of communication available ...   \n",
       "11                11  Robert Weber notes: \"To make valid inferences ...   \n",
       "12                12  There are five types of texts in content analy...   \n",
       "13                13  Over the years, content analysis has been appl...   \n",
       "14                14  In recent times, particularly with the advent ...   \n",
       "15                15  Quantitative content analysis has enjoyed a re...   \n",
       "16                16  Recently, Arash Heydarian Pashakhanlou has arg...   \n",
       "17                17  Content analysis can also be described as stud...   \n",
       "18                18  The method of content analysis enables the res...   \n",
       "19                19  Since the 1980s, content analysis has become a...   \n",
       "20                20  The creation of coding frames is intrinsically...   \n",
       "21                21  Mimetic Convergence aims to show the process o...   \n",
       "22                22  Every content analysis should depart from a hy...   \n",
       "23                23  As an evaluation approach, content analysis is...   \n",
       "24                24  Qualitative content analysis is \"a systematic,...   \n",
       "25                25  Holsti groups fifteen uses of content analysis...   \n",
       "26                26  He also places these uses into the context of ...   \n",
       "27                27  The following table shows fifteen uses of cont...   \n",
       "28                28                                                      \n",
       "29                 0  A document is a written, drawn, presented, or ...   \n",
       "..               ...                                                ...   \n",
       "50                 6  Relying on literary theory, the notion of text...   \n",
       "51                 0  Semantics (from Ancient Greek: σημαντικός sēma...   \n",
       "52                 1  In international scientific vocabulary semanti...   \n",
       "53                 2  The formal study of semantics intersects with ...   \n",
       "54                 3  Semantics contrasts with syntax, the study of ...   \n",
       "55                 4                                                      \n",
       "56                 5                                                      \n",
       "57                 6  In linguistics, semantics is the subfield that...   \n",
       "58                 7  In the late 1960s, Richard Montague proposed a...   \n",
       "59                 8  Despite its elegance, Montague grammar was lim...   \n",
       "60                 9  In Chomskyan linguistics there was no mechanis...   \n",
       "61                10  This view of semantics, as an innate finite me...   \n",
       "62                11  A concrete example of the latter phenomenon is...   \n",
       "63                12  Each of a set of synonyms like redouter ('to d...   \n",
       "64                13  and may go back to earlier Indian views on lan...   \n",
       "65                14  An attempt to defend a system based on proposi...   \n",
       "66                15  Another set of concepts related to fuzziness i...   \n",
       "67                16  Systems of categories are not objectively out ...   \n",
       "68                17  Originates from Montague's work (see above). A...   \n",
       "69                18  Pioneered by the philosopher Donald Davidson, ...   \n",
       "70                19  This theory is an effort to explain properties...   \n",
       "71                20  A linguistic theory that investigates word mea...   \n",
       "72                21  Computational semantics is focused on the proc...   \n",
       "73                22  In computer science, the term semantics refers...   \n",
       "74                23  The semantics of programming languages and oth...   \n",
       "75                24  For instance, the following statements use dif...   \n",
       "76                25  Various ways have been developed to describe t...   \n",
       "77                26  The Semantic Web refers to the extension of th...   \n",
       "78                27  In psychology, semantic memory is memory for m...   \n",
       "79                28  Ideasthesia is a psychological phenomenon in w...   \n",
       "\n",
       "                                               source source-paragraph-number  \\\n",
       "0      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "1      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "2      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "3      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "4      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "5      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "6      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "7      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "8      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "9      https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "10     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "11     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "12     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "13     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "14     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "15     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "16     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "17     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "18     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "19     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "20     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "21     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "22     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "23     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "24     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "25     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "26     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "27     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "28     https://en.wikipedia.org/wiki/Content_analysis                    None   \n",
       "29             https://en.wikipedia.org/wiki/Document                       1   \n",
       "..                                                ...                     ...   \n",
       "50  https://en.wikipedia.org/wiki/Text_(literary_t...                       2   \n",
       "51            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "52            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "53            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "54            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "55            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "56            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "57            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "58            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "59            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "60            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "61            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "62            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "63            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "64            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "65            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "66            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "67            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "68            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "69            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "70            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "71            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "72            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "73            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "74            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "75            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "76            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "77            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "78            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "79            https://en.wikipedia.org/wiki/Semantics                       2   \n",
       "\n",
       "   source-paragraph-text  \n",
       "0                   None  \n",
       "1                   None  \n",
       "2                   None  \n",
       "3                   None  \n",
       "4                   None  \n",
       "5                   None  \n",
       "6                   None  \n",
       "7                   None  \n",
       "8                   None  \n",
       "9                   None  \n",
       "10                  None  \n",
       "11                  None  \n",
       "12                  None  \n",
       "13                  None  \n",
       "14                  None  \n",
       "15                  None  \n",
       "16                  None  \n",
       "17                  None  \n",
       "18                  None  \n",
       "19                  None  \n",
       "20                  None  \n",
       "21                  None  \n",
       "22                  None  \n",
       "23                  None  \n",
       "24                  None  \n",
       "25                  None  \n",
       "26                  None  \n",
       "27                  None  \n",
       "28                  None  \n",
       "29             documents  \n",
       "..                   ...  \n",
       "50                 texts  \n",
       "51            meaningful  \n",
       "52            meaningful  \n",
       "53            meaningful  \n",
       "54            meaningful  \n",
       "55            meaningful  \n",
       "56            meaningful  \n",
       "57            meaningful  \n",
       "58            meaningful  \n",
       "59            meaningful  \n",
       "60            meaningful  \n",
       "61            meaningful  \n",
       "62            meaningful  \n",
       "63            meaningful  \n",
       "64            meaningful  \n",
       "65            meaningful  \n",
       "66            meaningful  \n",
       "67            meaningful  \n",
       "68            meaningful  \n",
       "69            meaningful  \n",
       "70            meaningful  \n",
       "71            meaningful  \n",
       "72            meaningful  \n",
       "73            meaningful  \n",
       "74            meaningful  \n",
       "75            meaningful  \n",
       "76            meaningful  \n",
       "77            meaningful  \n",
       "78            meaningful  \n",
       "79            meaningful  \n",
       "\n",
       "[80 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #ignore_index means the indices will not be reset after each append\n",
    "    contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">Section 2</span>\n",
    "<span style=\"color:red\">Construct cells immediately below this that spider webcontent from another site with content relating to your anticipated final project. Specifically, identify urls on a core page, then follow and extract content from them into a pandas `Dataframe`. In addition, demonstrate a *recursive* spider, which follows more than one level of links (i.e., follows links from a site, then follows links on followed sites to new sites, etc.), making sure to define a reasonable endpoint so that you do not wander the web forever :-).</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape the whole novel Battle Through the Heavens, I crawled the first two chapters on Hellotraslations via __[index](http://www.wuxiaworld.com/btth-index/)__ and the rest of it on Wuxiaworld via `Next Chapter button` on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the urls of Chapter 1, 2, and 3\n",
    "index_page = 'http://www.wuxiaworld.com/btth-index/'\n",
    "index_soup = prepare_soup(index_page)\n",
    "addresses = index_soup.find(id = \"target-id2970\").findAll('a')\n",
    "chapter1 = addresses[0].get('href')\n",
    "chapter2 = addresses[1].get('href')\n",
    "chapter3 = addresses[2].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct a function that reads the current soup and returns the url for next chapter\n",
    "def find_next_chapter(btth_soup):\n",
    "    try:\n",
    "        next_chapter = btth_soup.find('a', href = True, text = 'Next Chapter')['href']\n",
    "        return next_chapter\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the dataframes that will be populated\n",
    "paragraphDF = pandas.DataFrame()\n",
    "conversationDF = pandas.DataFrame()\n",
    "sentenceDF = pandas.DataFrame()\n",
    "\n",
    "# Construct a function that appends the dataframes collected from a single page to the accumulated ones\n",
    "def append_DF(paragraphDF, conversationDF, sentenceDF, new_para, new_conv, new_sent):\n",
    "    paragraphDF = paragraphDF.append(new_para)\n",
    "    conversationDF = conversationDF.append(new_conv)\n",
    "    sentenceDF = sentenceDF.append(new_sent)\n",
    "    return paragraphDF, conversationDF, sentenceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect paragraphs, conversations, and sentences of the novel Battle Through the Heavens by spidering all chapters\n",
    "\n",
    "## Collect Chapter 1\n",
    "chapter1_soup = prepare_soup(chapter1)\n",
    "chapter1_para, chapter1_conv, chapter1_sent = btth_scraper(chapter1, chapter1_soup)\n",
    "paragraphDF, conversationDF, sentenceDF = append_DF(paragraphDF, conversationDF, sentenceDF,\n",
    "                                                    chapter1_para, chapter1_conv, chapter1_sent)\n",
    "\n",
    "## Collect Chapter 2\n",
    "chapter2_soup = prepare_soup(chapter2)\n",
    "chapter2_para, chapter2_conv, chapter2_sent = btth_scraper(chapter2, chapter2_soup)\n",
    "paragraphDF, conversationDF, sentenceDF = append_DF(paragraphDF, conversationDF, sentenceDF,\n",
    "                                                    chapter2_para, chapter2_conv, chapter2_sent)\n",
    "\n",
    "chapter_url = chapter3\n",
    "\n",
    "'''\n",
    "## Collect the rest of the novel\n",
    "is_last_chapter = False\n",
    "while not is_last_chapter:\n",
    "    chapter_soup = prepare_soup(chapter_url)\n",
    "    chapter_para, chapter_conv, chapter_sent = btth_scraper(chapter_url, chapter_soup)\n",
    "    paragraphDF, conversationDF, sentenceDF = append_DF(paragraphDF, conversationDF, sentenceDF,\n",
    "                                                        chapter_para, chapter_conv, chapter_sent)\n",
    "    chapter_url = find_next_chapter(chapter_soup)\n",
    "    if chapter_url == None:\n",
    "        is_last_chapter = True\n",
    "'''\n",
    "    \n",
    "## Collect Chapter 3 - 200\n",
    "i = 3\n",
    "while i <= 200:\n",
    "    chapter_soup = prepare_soup(chapter_url)\n",
    "    chapter_para, chapter_conv, chapter_sent = btth_scraper(chapter_url, chapter_soup)\n",
    "    paragraphDF, conversationDF, sentenceDF = append_DF(paragraphDF, conversationDF, sentenceDF,\n",
    "                                                        chapter_para, chapter_conv, chapter_sent)\n",
    "    chapter_url = find_next_chapter(chapter_soup)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            paragraph\n",
      "0                           'Dou Zhi Li1, 3rd stage!'\n",
      "1   Facing the Magical Testing Monument as it disp...\n",
      "2   \"Xiao Yan, Dou Zhi Li, 3rd stage! Rank: Low!\" ...\n",
      "3   Immediately after the middle-aged man finished...\n",
      "4   \"3rd stage? Hmmhmm, as expected. This 'genius'...\n",
      "5   \"Ai, this piece of trash really disgraced his ...\n",
      "6   \"If his father wasn't the clan leader, this ki...\n",
      "7   \"How could the once famous genius of Wu Tan Ch...\n",
      "8   \"Who knows? Maybe he did something unforgivabl...\n",
      "9   The ridicule and laughter directed at him came...\n",
      "10  He raised his head to reveal a delicate and im...\n",
      "11  \"These people, were they always this cold? Or ...\n",
      "12                               \"Next up, Xiao Mei!\"\n",
      "13  Hearing the tester's voice, a teenage girl qui...\n",
      "14  The teenage girl wasn't more than 14 years old...\n",
      "15  She quickly stepped forward and put her tiny h...\n",
      "16  As the girl closed her eyes, the monument shin...\n",
      "17                           'Dou Zhi Li, 7th stage!'\n",
      "18     \"Xiao Mei, Dou Zhi Li, 7th stage! Rank: High!\"\n",
      "19  \"Yeh!\" Hearing the tester read out the result,...\n",
      "20  \"Tsk tsk, 7th stage Dou Zhi Li. How impressive...\n",
      "21  \"She really fits the name of the clan's seedli...\n",
      "22  Hearing the unanimous praise coming from the c...\n",
      "23  While chatting with her fellow sister members,...\n",
      "24  Furrowing her brows for a while, she finally d...\n",
      "25  \"Ai…\" An inexplicable sigh emerged from her. X...\n",
      "26  At that time, his self-confidence as well as i...\n",
      "27  However, the path of a genius was always windi...\n",
      "28  As a result of losing his Dou Zhi Li, his phys...\n",
      "29  From the position of a genius, in one night he...\n",
      "..                                                ...\n",
      "28  \"If there isn't, then we'll just have to conti...\n",
      "29  \"But… but I spent so much effort… do I just en...\n",
      "30  \"In this world, there are thousands and thousa...\n",
      "31                               \"Huh? What is this?\"\n",
      "32  As his gaze swept the lotus platform in detail...\n",
      "33  \"What is this thing?\" Tilting his head in a li...\n",
      "34  The thing in Yao Lao's hand was about half a p...\n",
      "35  \"This is… a Seven-Colored Snake Scale?\" Yao La...\n",
      "36                       \"Seven-Colored Snake Scale?\"\n",
      "37  \"I was wondering why there isn't any 'Green Lo...\n",
      "38  Xiao Yan received the scale and felt the spot ...\n",
      "39  \"Did the owner of this scale take the 'Green L...\n",
      "40  \"There is only one person in the entire Tager ...\n",
      "41  \"Ugh. So what if it is her? It has been half a...\n",
      "42  \"You have really become muddle headed… Queen M...\n",
      "43  \"Since her attributes clash with the 'Heavenly...\n",
      "44  \"That may be so…\" Yao Lao waved his hand and v...\n",
      "45  \"Go and find Queen Medusa to snatch the 'Heave...\n",
      "46  \"Having a target is better than us randomly ru...\n",
      "47  Xiao Yan bitterly smiled and could only sigh a...\n",
      "48  \"Hey, what are you doing?\" Seeing Xiao Yan's a...\n",
      "49  \"Going back… Should I stay here and eat magma?...\n",
      "50  \"You… you fool.\" Hearing the words, Yao Lao wa...\n",
      "51  \"What?\" Xiao Yan blinked his eyes in shock. He...\n",
      "52  Rolling his eyes, Yao Lao had difficulty swall...\n",
      "53  \"And there's the lotus seeds in the lotus plat...\n",
      "54  \"If you take any of these things out, you woul...\n",
      "55  \"…\" Hearing the mouthwatering explanation of Y...\n",
      "56          \"Dammit, consider all this all interest…\"\n",
      "57                      Previous Chapter Next Chapter\n",
      "\n",
      "[9844 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         conversation\n",
      "0       \"Xiao Yan, Dou Zhi Li, 3rd stage! Rank: Low!\"\n",
      "1   \"3rd stage? Hmmhmm, as expected. This 'genius'...\n",
      "2   \"Ai, this piece of trash really disgraced his ...\n",
      "3   \"If his father wasn't the clan leader, this ki...\n",
      "4   \"How could the once famous genius of Wu Tan Ch...\n",
      "5   \"Who knows? Maybe he did something unforgivabl...\n",
      "6   \"These people, were they always this cold? Or ...\n",
      "7                                \"Next up, Xiao Mei!\"\n",
      "8      \"Xiao Mei, Dou Zhi Li, 7th stage! Rank: High!\"\n",
      "9                                              \"Yeh!\"\n",
      "10  \"Tsk tsk, 7th stage Dou Zhi Li. How impressive...\n",
      "11  \"She really fits the name of the clan's seedli...\n",
      "12                                              \"Ai…\"\n",
      "13                               \"Next, Xiao Xun Er!\"\n",
      "14               \"Dou Zhi Li, 9th stage! Rank: High!\"\n",
      "15  \"…… She really reached the 9th stage, how frig...\n",
      "16                                          \"Thanks.\"\n",
      "17                                \"Brother Xiao Yan.\"\n",
      "18  \"What qualifications do I have right now for y...\n",
      "19  \"Brother Xiao Yan, you once said to Xun Er6 be...\n",
      "20  \"Haha, truly free? I only know how to say it. ...\n",
      "21  \"Brother Xiao Yan, though I don't know what's ...\n",
      "22                                            \"Haha…\"\n",
      "0                                               \"Ai…\"\n",
      "1                            \"15 years already, huh?\"\n",
      "2   \"God damn it! How could I be played for a fool...\n",
      "3   \"These few years, I've really let Mother down,...\n",
      "4                        \"Father, why have you come?\"\n",
      "5   \"Ah ah, Yan Er3. It's already quite late. Why ...\n",
      "6    \"Father, why have you not returned to rest yet?\"\n",
      "..                                                ...\n",
      "8   \"Let's go. Be careful as you go over. Looking ...\n",
      "9                                              \"Yes.\"\n",
      "10  \"How can this be? Why isn't it there? From the...\n",
      "11                              \"Why is it not here?\"\n",
      "12                            \"Quiet! Calm down now!\"\n",
      "13  \"If there isn't, then we'll just have to conti...\n",
      "14  \"But… but I spent so much effort… do I just en...\n",
      "15  \"In this world, there are thousands and thousa...\n",
      "16                               \"Huh? What is this?\"\n",
      "17  \"What is this thing?\" Tilting his head in a li...\n",
      "18            \"This is… a Seven-Colored Snake Scale?\"\n",
      "19                       \"Seven-Colored Snake Scale?\"\n",
      "20  \"I was wondering why there isn't any 'Green Lo...\n",
      "21  \"Did the owner of this scale take the 'Green L...\n",
      "22  \"There is only one person in the entire Tager ...\n",
      "23  \"Ugh. So what if it is her? It has been half a...\n",
      "24  \"You have really become muddle headed… Queen M...\n",
      "25  \"Since her attributes clash with the 'Heavenly...\n",
      "26  \"That may be so…\" Yao Lao waved his hand and v...\n",
      "27  \"Go and find Queen Medusa to snatch the 'Heave...\n",
      "28  \"Having a target is better than us randomly ru...\n",
      "29                         \"Hey, what are you doing?\"\n",
      "30    \"Going back… Should I stay here and eat magma?\"\n",
      "31  \"You… you fool.\" Hearing the words, Yao Lao wa...\n",
      "32  \"What?\" Xiao Yan blinked his eyes in shock. He...\n",
      "33  \"This green colored lotus platform is formed f...\n",
      "34  \"And there's the lotus seeds in the lotus plat...\n",
      "35  \"If you take any of these things out, you woul...\n",
      "36                                                \"…\"\n",
      "37          \"Dammit, consider all this all interest…\"\n",
      "\n",
      "[4534 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(conversationDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence\n",
      "0                              Dou Zhi Li1, 3rd stage!\n",
      "1    Facing the Magical Testing Monument as it disp...\n",
      "2    He tightly clenched his fist and because of th...\n",
      "3                     Xiao Yan, Dou Zhi Li, 3rd stage!\n",
      "4                                           Rank: Low!\n",
      "5    Beside the Magical Testing Monument, a middle-...\n",
      "6    Immediately after the middle-aged man finished...\n",
      "7                                           3rd stage?\n",
      "8                                 Hmmhmm, as expected.\n",
      "9      This 'genius' has once again taken a step back.\n",
      "10   Ai, this piece of trash really disgraced his e...\n",
      "11   If his father wasn't the clan leader, this kin...\n",
      "12   Nobody would care about him, and there wouldn'...\n",
      "13   How could the once famous genius of Wu Tan Che...\n",
      "14                                          Who knows?\n",
      "15   Maybe he did something unforgivable and caused...\n",
      "16   The ridicule and laughter directed at him came...\n",
      "17              He could not help but breathe heavily.\n",
      "18   He raised his head to reveal a delicate and im...\n",
      "19   His lips, which were previously full of self-r...\n",
      "20           These people, were they always this cold?\n",
      "21   Or was it because three years ago they smiled ...\n",
      "22   Smiling bitterly, Xiao Yan turned around and s...\n",
      "23   His lonesome figure seemed to be unable to fit...\n",
      "24                                  Next up, Xiao Mei!\n",
      "25   Hearing the tester's voice, a teenage girl qui...\n",
      "26   The moment she got on stage, the murmurings an...\n",
      "27     The teenage girl wasn't more than 14 years old.\n",
      "28   Although her beauty wasn't immediately evident...\n",
      "29   She quickly stepped forward and put her tiny h...\n",
      "..                                                 ...\n",
      "141  Although the fierce name of Queen Medusa is re...\n",
      "142                  Yao Lao curled his lips and said.\n",
      "143  Xiao Yan bitterly smiled and could only sigh a...\n",
      "144  Immediately, his head turned with his body and...\n",
      "145                           Hey, what are you doing?\n",
      "146  Seeing Xiao Yan's action, Yao Lao appeared som...\n",
      "147      Going back… Should I stay here and eat magma?\n",
      "148                       Xiao Yan impolitely replied.\n",
      "149                                     You… you fool.\n",
      "150  Hearing the words, Yao Lao was immediately ang...\n",
      "151  His finger pointed to the green colored lotus ...\n",
      "152                 You child want to abandon it here?\n",
      "153                                              What?\n",
      "154                Xiao Yan blinked his eyes in shock.\n",
      "155  He turned around and eyed the green colored lo...\n",
      "156  Rolling his eyes, Yao Lao had difficulty swall...\n",
      "157  He drew his beard and said irritably, \"This gr...\n",
      "158  As long as you remove it and sit on it to trai...\n",
      "159  Moreover, you can activate it using your Dou Q...\n",
      "160  If you meet a person at the Dou Ling level, yo...\n",
      "161  And there's the lotus seeds in the lotus platf...\n",
      "162  This Core-Fire Lotus Seed that is known as the...\n",
      "163  If you go out and shout that you have a Core-F...\n",
      "164  Of course… this is excluding those who prefer ...\n",
      "165  If you take any of these things out, you would...\n",
      "166                       You child dare to refuse it?\n",
      "167  Having spoken until this point, Yao Lao spoke ...\n",
      "168  Hearing the mouthwatering explanation of Yao L...\n",
      "169  By the time Yao Lao had finished, Xiao Yan's e...\n",
      "170            Dammit, consider all this all interest…\n",
      "\n",
      "[23537 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sentenceDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API (Tumblr)\n",
    "\n",
    "Generally website owners do not like you scraping their sites. If done badly,\n",
    "scarping can act like a DOS attack so you should be careful how often you make\n",
    "calls to a site. Some sites want automated tools to access their data, so they\n",
    "create [application programming interface\n",
    "(APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). An API\n",
    "specifies a procedure for an application (or script) to access their data. Often\n",
    "this is though a [representational state transfer\n",
    "(REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web\n",
    "service, which just means if you make correctly formatted HTTP requests they\n",
    "will return nicely formatted data.\n",
    "\n",
    "A nice example for us to study is [Tumblr](https://www.tumblr.com), they have a\n",
    "[simple RESTful API](https://www.tumblr.com/docs/en/api/v1) that allows you to\n",
    "read posts without any complicated html parsing.\n",
    "\n",
    "We can get the first 20 posts from a blog by making an http GET request to\n",
    "`'http://{blog}.tumblr.com/api/read/json'`, were `{blog}` is the name of the\n",
    "target blog. Lets try and get the posts from [http://lolcats-lol-\n",
    "cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Note the blog says at the\n",
    "top 'One hour one pic lolcats', but the canonical name that Tumblr uses is in\n",
    "the URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tumblr_api_read = {\"tumblelog\":{\"title\":\"One hour one pic lolcats\",\"description\":\"\",\"name\":\"lolcats-lol-cat\",\"timezone\":\"Europe\\/Paris\",\"cname\":false,\"feeds\":[]},\"posts-start\":0,\"posts-total\":2964,\"posts-type\":false,\"posts\":[{\"id\":\"169306612050\",\"url\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/169306612050\",\"url-with-slug\":\"http:\\/\\/lolcats-lol-cat.tumblr.com\\/post\\/169306612050\\/kitty-mind-blown\",\"type\":\"photo\",\"date-gmt\":\"2018-01-04 15:00:13 GMT\",\"date\":\"Thu, 04 Jan 2018 16:00:13\",\"bookmarklet\":0,\"mobile\":0,\"feed-item\":\"\",\"from-feed-id\":0,\"unix-timestamp\":1515078013,\"format\":\"html\",\"reblog-key\":\"TZzKhxon\",\"slug\":\"kitty-mind-blown\",\"is-submission\":false,\"like-button\":\"<div class=\\\"like_button\\\" data-post-id=\\\"169306612050\\\" data-blog-name=\\\"lolcats-lol-cat\\\" id=\\\"like_button_169306612050\\\"><iframe id=\\\"like_iframe_169306612050\\\" src=\\\"http:\\/\\/assets.tumblr.com\\/assets\\/html\\/like_iframe.html?_v=fc298e85f978b8662a643fe0a6b8c638#name=lolcats-lol-cat&amp;post_id=169306612050&amp;co\n"
     ]
    }
   ],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might not look very good on first inspection, but it has far fewer angle\n",
    "braces than html, which makes it easier to parse. What we have is\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) a 'human readable' text based data\n",
    "transmission format based on javascript. Luckily, we can readily convert it to a\n",
    "python `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['posts', 'tumblelog', 'posts-total', 'posts-type', 'posts-start'])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#We need to load only the stuff between the curly braces\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we read the [API specification](https://www.tumblr.com/docs/en/api/v1), we\n",
    "will see there are a lot of things we can get if we add things to our GET\n",
    "request. First we can retrieve posts by their id number. Let's first get post\n",
    "`146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Such beauty; such vigor (If you can't see it you have to refresh the page). Now\n",
    "we could retrieve the text from all posts as well\n",
    "as related metadata, like the post date, caption or tags. We could also get\n",
    "links to all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>photo-type</th>\n",
       "      <th>photo-url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu, 04 Jan 2018 16:00:13</td>\n",
       "      <td>169306612050</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/704a681944a94a1fac1...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu, 04 Jan 2018 14:00:41</td>\n",
       "      <td>169303827051</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/1ec2cc33b059d657351...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu, 04 Jan 2018 12:00:34</td>\n",
       "      <td>169301536211</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/80dc7c42c35a2dbce58...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, 02 Jan 2018 04:00:09</td>\n",
       "      <td>169209300706</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/94b8dbe91683bd01551...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue, 02 Jan 2018 02:00:42</td>\n",
       "      <td>169205268941</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/e3bbb26992ac10e0234...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sun, 31 Dec 2017 08:00:10</td>\n",
       "      <td>169141076975</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/5ef1ee635831085377d...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sun, 31 Dec 2017 06:00:35</td>\n",
       "      <td>169137806653</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/9dc91b972231d1e6250...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sun, 31 Dec 2017 04:00:38</td>\n",
       "      <td>169134323015</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/f62a86855bb0ed8bf1e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sun, 31 Dec 2017 02:00:42</td>\n",
       "      <td>169130694997</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/760cd9b9017708c90d5...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sun, 24 Dec 2017 08:00:08</td>\n",
       "      <td>168883132580</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/c078fc21432061a41ec...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sun, 24 Dec 2017 06:00:08</td>\n",
       "      <td>168880019803</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/4c15fa80a9693e21cd2...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sun, 24 Dec 2017 04:00:37</td>\n",
       "      <td>168876801090</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/41e768a2598ceff3589...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sun, 24 Dec 2017 02:00:46</td>\n",
       "      <td>168873615432</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/be75972f1c67a08a617...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wed, 20 Dec 2017 16:00:18</td>\n",
       "      <td>168752469243</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/00f9975dfb0fefcc17f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wed, 20 Dec 2017 14:00:31</td>\n",
       "      <td>168749896972</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/8210828c1144bff07d0...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wed, 20 Dec 2017 12:00:37</td>\n",
       "      <td>168747789963</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/263e3e9a5416485ce12...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wed, 20 Dec 2017 10:00:30</td>\n",
       "      <td>168745836988</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/7da179e266179512629...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wed, 20 Dec 2017 08:00:27</td>\n",
       "      <td>168743505623</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/452a57d8576fa652d4b...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wed, 20 Dec 2017 06:00:25</td>\n",
       "      <td>168740395058</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/a5e6c94f65fb71e75b4...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Wed, 20 Dec 2017 04:00:27</td>\n",
       "      <td>168736855161</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/ffac83e4af480b33c91...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Thu, 14 Dec 2017 08:00:17</td>\n",
       "      <td>168529363372</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/79fb3620a0fcc58b945...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Thu, 14 Dec 2017 06:00:10</td>\n",
       "      <td>168526308691</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/031b538d73b86cd7d2e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Thu, 14 Dec 2017 04:00:13</td>\n",
       "      <td>168522893885</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/0da9dfd02dfe53c865e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Thu, 14 Dec 2017 02:00:15</td>\n",
       "      <td>168519442371</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/62dc277cc5b667833eb...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sat, 09 Dec 2017 20:00:23</td>\n",
       "      <td>168364451223</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/50ab257f3405612a14a...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sat, 09 Dec 2017 18:00:24</td>\n",
       "      <td>168361168241</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/f88052384b7580650e6...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sat, 09 Dec 2017 16:00:39</td>\n",
       "      <td>168358136361</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/bf923d1a4b17f98439b...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sat, 09 Dec 2017 14:00:19</td>\n",
       "      <td>168355590755</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/49c3f5ba6ba4e479a74...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sat, 09 Dec 2017 12:00:17</td>\n",
       "      <td>168353555383</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/f842d7761f5c117e672...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sat, 09 Dec 2017 10:00:13</td>\n",
       "      <td>168351653830</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/43d75634d94069c7137...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sat, 09 Dec 2017 08:00:11</td>\n",
       "      <td>168349411113</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/d025e5c20c1988cc0ce...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sat, 09 Dec 2017 06:00:11</td>\n",
       "      <td>168346531295</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/555af6d416083a47906...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mon, 04 Dec 2017 22:00:40</td>\n",
       "      <td>168195598603</td>\n",
       "      <td>gif</td>\n",
       "      <td>http://78.media.tumblr.com/25e88dde076cbc1c970...</td>\n",
       "      <td>[gif, lolcat, lolcats, cat, funny, 90s, vaporw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sun, 03 Dec 2017 20:00:23</td>\n",
       "      <td>168155503393</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/497bf2459417cff68ed...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sun, 03 Dec 2017 18:00:31</td>\n",
       "      <td>168151885936</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/183183694ee7c69f83e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Fri, 01 Dec 2017 10:00:34</td>\n",
       "      <td>168073544580</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/504f0f2a046c5f50788...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Fri, 01 Dec 2017 08:00:32</td>\n",
       "      <td>168071375123</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/a2f9da10bba1d62da49...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Fri, 01 Dec 2017 04:00:33</td>\n",
       "      <td>168065028458</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/6a4266d994e5fdc443f...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Thu, 30 Nov 2017 18:00:39</td>\n",
       "      <td>168048588399</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/dcd00e008f3b004ff7e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Thu, 30 Nov 2017 02:00:20</td>\n",
       "      <td>168027714962</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/41114219abdabd67401...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Wed, 29 Nov 2017 22:00:52</td>\n",
       "      <td>168020830712</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/b93757bd30a75d72194...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Wed, 29 Nov 2017 20:00:41</td>\n",
       "      <td>168017539355</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/15c96d570c09534f110...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Wed, 29 Nov 2017 18:00:44</td>\n",
       "      <td>168014439902</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/8b322c4b6c082e10b92...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Wed, 29 Nov 2017 10:00:33</td>\n",
       "      <td>168005156728</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/de3485e2b6a476f6c14...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Wed, 29 Nov 2017 08:00:31</td>\n",
       "      <td>168002971885</td>\n",
       "      <td>png</td>\n",
       "      <td>http://78.media.tumblr.com/127ec0a380edc6a927e...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Tue, 28 Nov 2017 08:00:11</td>\n",
       "      <td>167968422430</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/0ff6d016269ffe480e2...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mon, 27 Nov 2017 22:00:46</td>\n",
       "      <td>167951611533</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/260a690dd41167d5cfb...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Mon, 27 Nov 2017 18:00:41</td>\n",
       "      <td>167945064894</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/182f0314ab24b11c0df...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Mon, 27 Nov 2017 06:00:24</td>\n",
       "      <td>167930309171</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/f29fe0a3b656b808200...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sun, 26 Nov 2017 12:00:26</td>\n",
       "      <td>167900284395</td>\n",
       "      <td>jpg</td>\n",
       "      <td>http://78.media.tumblr.com/3ed49d55ba8dcec08f0...</td>\n",
       "      <td>[cat, cats, lol, lolcat, lolcats]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date            id photo-type  \\\n",
       "0   Thu, 04 Jan 2018 16:00:13  169306612050        jpg   \n",
       "1   Thu, 04 Jan 2018 14:00:41  169303827051        png   \n",
       "2   Thu, 04 Jan 2018 12:00:34  169301536211        png   \n",
       "3   Tue, 02 Jan 2018 04:00:09  169209300706        jpg   \n",
       "4   Tue, 02 Jan 2018 02:00:42  169205268941        jpg   \n",
       "5   Sun, 31 Dec 2017 08:00:10  169141076975        png   \n",
       "6   Sun, 31 Dec 2017 06:00:35  169137806653        jpg   \n",
       "7   Sun, 31 Dec 2017 04:00:38  169134323015        png   \n",
       "8   Sun, 31 Dec 2017 02:00:42  169130694997        jpg   \n",
       "9   Sun, 24 Dec 2017 08:00:08  168883132580        jpg   \n",
       "10  Sun, 24 Dec 2017 06:00:08  168880019803        jpg   \n",
       "11  Sun, 24 Dec 2017 04:00:37  168876801090        jpg   \n",
       "12  Sun, 24 Dec 2017 02:00:46  168873615432        jpg   \n",
       "13  Wed, 20 Dec 2017 16:00:18  168752469243        jpg   \n",
       "14  Wed, 20 Dec 2017 14:00:31  168749896972        jpg   \n",
       "15  Wed, 20 Dec 2017 12:00:37  168747789963        jpg   \n",
       "16  Wed, 20 Dec 2017 10:00:30  168745836988        jpg   \n",
       "17  Wed, 20 Dec 2017 08:00:27  168743505623        jpg   \n",
       "18  Wed, 20 Dec 2017 06:00:25  168740395058        jpg   \n",
       "19  Wed, 20 Dec 2017 04:00:27  168736855161        jpg   \n",
       "20  Thu, 14 Dec 2017 08:00:17  168529363372        jpg   \n",
       "21  Thu, 14 Dec 2017 06:00:10  168526308691        jpg   \n",
       "22  Thu, 14 Dec 2017 04:00:13  168522893885        jpg   \n",
       "23  Thu, 14 Dec 2017 02:00:15  168519442371        jpg   \n",
       "24  Sat, 09 Dec 2017 20:00:23  168364451223        jpg   \n",
       "25  Sat, 09 Dec 2017 18:00:24  168361168241        png   \n",
       "26  Sat, 09 Dec 2017 16:00:39  168358136361        jpg   \n",
       "27  Sat, 09 Dec 2017 14:00:19  168355590755        jpg   \n",
       "28  Sat, 09 Dec 2017 12:00:17  168353555383        jpg   \n",
       "29  Sat, 09 Dec 2017 10:00:13  168351653830        jpg   \n",
       "30  Sat, 09 Dec 2017 08:00:11  168349411113        jpg   \n",
       "31  Sat, 09 Dec 2017 06:00:11  168346531295        jpg   \n",
       "32  Mon, 04 Dec 2017 22:00:40  168195598603        gif   \n",
       "33  Sun, 03 Dec 2017 20:00:23  168155503393        jpg   \n",
       "34  Sun, 03 Dec 2017 18:00:31  168151885936        png   \n",
       "35  Fri, 01 Dec 2017 10:00:34  168073544580        jpg   \n",
       "36  Fri, 01 Dec 2017 08:00:32  168071375123        jpg   \n",
       "37  Fri, 01 Dec 2017 04:00:33  168065028458        jpg   \n",
       "38  Thu, 30 Nov 2017 18:00:39  168048588399        jpg   \n",
       "39  Thu, 30 Nov 2017 02:00:20  168027714962        jpg   \n",
       "40  Wed, 29 Nov 2017 22:00:52  168020830712        jpg   \n",
       "41  Wed, 29 Nov 2017 20:00:41  168017539355        jpg   \n",
       "42  Wed, 29 Nov 2017 18:00:44  168014439902        jpg   \n",
       "43  Wed, 29 Nov 2017 10:00:33  168005156728        jpg   \n",
       "44  Wed, 29 Nov 2017 08:00:31  168002971885        png   \n",
       "45  Tue, 28 Nov 2017 08:00:11  167968422430        jpg   \n",
       "46  Mon, 27 Nov 2017 22:00:46  167951611533        jpg   \n",
       "47  Mon, 27 Nov 2017 18:00:41  167945064894        jpg   \n",
       "48  Mon, 27 Nov 2017 06:00:24  167930309171        jpg   \n",
       "49  Sun, 26 Nov 2017 12:00:26  167900284395        jpg   \n",
       "\n",
       "                                            photo-url  \\\n",
       "0   http://78.media.tumblr.com/704a681944a94a1fac1...   \n",
       "1   http://78.media.tumblr.com/1ec2cc33b059d657351...   \n",
       "2   http://78.media.tumblr.com/80dc7c42c35a2dbce58...   \n",
       "3   http://78.media.tumblr.com/94b8dbe91683bd01551...   \n",
       "4   http://78.media.tumblr.com/e3bbb26992ac10e0234...   \n",
       "5   http://78.media.tumblr.com/5ef1ee635831085377d...   \n",
       "6   http://78.media.tumblr.com/9dc91b972231d1e6250...   \n",
       "7   http://78.media.tumblr.com/f62a86855bb0ed8bf1e...   \n",
       "8   http://78.media.tumblr.com/760cd9b9017708c90d5...   \n",
       "9   http://78.media.tumblr.com/c078fc21432061a41ec...   \n",
       "10  http://78.media.tumblr.com/4c15fa80a9693e21cd2...   \n",
       "11  http://78.media.tumblr.com/41e768a2598ceff3589...   \n",
       "12  http://78.media.tumblr.com/be75972f1c67a08a617...   \n",
       "13  http://78.media.tumblr.com/00f9975dfb0fefcc17f...   \n",
       "14  http://78.media.tumblr.com/8210828c1144bff07d0...   \n",
       "15  http://78.media.tumblr.com/263e3e9a5416485ce12...   \n",
       "16  http://78.media.tumblr.com/7da179e266179512629...   \n",
       "17  http://78.media.tumblr.com/452a57d8576fa652d4b...   \n",
       "18  http://78.media.tumblr.com/a5e6c94f65fb71e75b4...   \n",
       "19  http://78.media.tumblr.com/ffac83e4af480b33c91...   \n",
       "20  http://78.media.tumblr.com/79fb3620a0fcc58b945...   \n",
       "21  http://78.media.tumblr.com/031b538d73b86cd7d2e...   \n",
       "22  http://78.media.tumblr.com/0da9dfd02dfe53c865e...   \n",
       "23  http://78.media.tumblr.com/62dc277cc5b667833eb...   \n",
       "24  http://78.media.tumblr.com/50ab257f3405612a14a...   \n",
       "25  http://78.media.tumblr.com/f88052384b7580650e6...   \n",
       "26  http://78.media.tumblr.com/bf923d1a4b17f98439b...   \n",
       "27  http://78.media.tumblr.com/49c3f5ba6ba4e479a74...   \n",
       "28  http://78.media.tumblr.com/f842d7761f5c117e672...   \n",
       "29  http://78.media.tumblr.com/43d75634d94069c7137...   \n",
       "30  http://78.media.tumblr.com/d025e5c20c1988cc0ce...   \n",
       "31  http://78.media.tumblr.com/555af6d416083a47906...   \n",
       "32  http://78.media.tumblr.com/25e88dde076cbc1c970...   \n",
       "33  http://78.media.tumblr.com/497bf2459417cff68ed...   \n",
       "34  http://78.media.tumblr.com/183183694ee7c69f83e...   \n",
       "35  http://78.media.tumblr.com/504f0f2a046c5f50788...   \n",
       "36  http://78.media.tumblr.com/a2f9da10bba1d62da49...   \n",
       "37  http://78.media.tumblr.com/6a4266d994e5fdc443f...   \n",
       "38  http://78.media.tumblr.com/dcd00e008f3b004ff7e...   \n",
       "39  http://78.media.tumblr.com/41114219abdabd67401...   \n",
       "40  http://78.media.tumblr.com/b93757bd30a75d72194...   \n",
       "41  http://78.media.tumblr.com/15c96d570c09534f110...   \n",
       "42  http://78.media.tumblr.com/8b322c4b6c082e10b92...   \n",
       "43  http://78.media.tumblr.com/de3485e2b6a476f6c14...   \n",
       "44  http://78.media.tumblr.com/127ec0a380edc6a927e...   \n",
       "45  http://78.media.tumblr.com/0ff6d016269ffe480e2...   \n",
       "46  http://78.media.tumblr.com/260a690dd41167d5cfb...   \n",
       "47  http://78.media.tumblr.com/182f0314ab24b11c0df...   \n",
       "48  http://78.media.tumblr.com/f29fe0a3b656b808200...   \n",
       "49  http://78.media.tumblr.com/3ed49d55ba8dcec08f0...   \n",
       "\n",
       "                                                 tags  \n",
       "0                   [cat, cats, lol, lolcat, lolcats]  \n",
       "1                   [cat, cats, lol, lolcat, lolcats]  \n",
       "2                   [cat, cats, lol, lolcat, lolcats]  \n",
       "3                   [cat, cats, lol, lolcat, lolcats]  \n",
       "4                   [cat, cats, lol, lolcat, lolcats]  \n",
       "5                   [cat, cats, lol, lolcat, lolcats]  \n",
       "6                   [cat, cats, lol, lolcat, lolcats]  \n",
       "7                   [cat, cats, lol, lolcat, lolcats]  \n",
       "8                   [cat, cats, lol, lolcat, lolcats]  \n",
       "9                   [cat, cats, lol, lolcat, lolcats]  \n",
       "10                  [cat, cats, lol, lolcat, lolcats]  \n",
       "11                  [cat, cats, lol, lolcat, lolcats]  \n",
       "12                  [cat, cats, lol, lolcat, lolcats]  \n",
       "13                  [cat, cats, lol, lolcat, lolcats]  \n",
       "14                  [cat, cats, lol, lolcat, lolcats]  \n",
       "15                  [cat, cats, lol, lolcat, lolcats]  \n",
       "16                  [cat, cats, lol, lolcat, lolcats]  \n",
       "17                  [cat, cats, lol, lolcat, lolcats]  \n",
       "18                  [cat, cats, lol, lolcat, lolcats]  \n",
       "19                  [cat, cats, lol, lolcat, lolcats]  \n",
       "20                  [cat, cats, lol, lolcat, lolcats]  \n",
       "21                  [cat, cats, lol, lolcat, lolcats]  \n",
       "22                  [cat, cats, lol, lolcat, lolcats]  \n",
       "23                  [cat, cats, lol, lolcat, lolcats]  \n",
       "24                  [cat, cats, lol, lolcat, lolcats]  \n",
       "25                  [cat, cats, lol, lolcat, lolcats]  \n",
       "26                  [cat, cats, lol, lolcat, lolcats]  \n",
       "27                  [cat, cats, lol, lolcat, lolcats]  \n",
       "28                  [cat, cats, lol, lolcat, lolcats]  \n",
       "29                  [cat, cats, lol, lolcat, lolcats]  \n",
       "30                  [cat, cats, lol, lolcat, lolcats]  \n",
       "31                  [cat, cats, lol, lolcat, lolcats]  \n",
       "32  [gif, lolcat, lolcats, cat, funny, 90s, vaporw...  \n",
       "33                  [cat, cats, lol, lolcat, lolcats]  \n",
       "34                  [cat, cats, lol, lolcat, lolcats]  \n",
       "35                  [cat, cats, lol, lolcat, lolcats]  \n",
       "36                  [cat, cats, lol, lolcat, lolcats]  \n",
       "37                  [cat, cats, lol, lolcat, lolcats]  \n",
       "38                  [cat, cats, lol, lolcat, lolcats]  \n",
       "39                  [cat, cats, lol, lolcat, lolcats]  \n",
       "40                  [cat, cats, lol, lolcat, lolcats]  \n",
       "41                  [cat, cats, lol, lolcat, lolcats]  \n",
       "42                  [cat, cats, lol, lolcat, lolcats]  \n",
       "43                  [cat, cats, lol, lolcat, lolcats]  \n",
       "44                  [cat, cats, lol, lolcat, lolcats]  \n",
       "45                  [cat, cats, lol, lolcat, lolcats]  \n",
       "46                  [cat, cats, lol, lolcat, lolcats]  \n",
       "47                  [cat, cats, lol, lolcat, lolcats]  \n",
       "48                  [cat, cats, lol, lolcat, lolcats]  \n",
       "49                  [cat, cats, lol, lolcat, lolcats]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Putting a max in case the blog has millions of images\n",
    "#The given max will be rounded up to the nearest multiple of 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    #Restating this here so the function isn't dependent on any external variables\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "    #There are a bunch of possible locations for the photo url\n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #These are the pieces of information we will be gathering,\n",
    "    #at the end we will convert this to a DataFrame.\n",
    "    #There are a  few other datums we could gather like the captions\n",
    "    #you can read the Tumblr documentation to learn how to get them\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Tumblr limits us to a max of 50 posts per request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #We are dealing with uncleaned data, we can't trust it.\n",
    "            #Specifically, not all posts are guaranteed to have the fields we want\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Make sure your error messages are useful\n",
    "                #You will be one of the users\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the urls of a bunch of images and can run OCR on them to gather\n",
    "compelling meme narratives, accompanied by cats.\n",
    "\n",
    "# Files\n",
    "\n",
    "What if the text we want isn't on a webpage? There are a many other sources of\n",
    "text available, typically organized into *files*.\n",
    "\n",
    "## Raw text (and encoding)\n",
    "\n",
    "The most basic form of storing text is as a _raw text_ document. Source code\n",
    "(`.py`, `.r`, etc) is usually raw text as are text files (`.txt`) and those with\n",
    "many other extension (e.g., .csv, .dat, etc.). Opening an unknown file with a\n",
    "text editor is often a great way of learning what the file is.\n",
    "\n",
    "We can create a text file in python with the `open()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, mode = 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `encoding='utf-8'` argument, which specifies how we map the bits from\n",
    "the file to the glyphs (and whitespace characters like tab (`'\\t'`) or newline\n",
    "(`'\\n'`)) on the screen. When dealing only with latin letters, arabic numerals\n",
    "and the other symbols on America keyboards you usually do not have to worry\n",
    "about encodings as the ones used today are backwards compatible with\n",
    "[ASCII](https://en.wikipedia.org/wiki/ASCII), which gives the binary\n",
    "representation of 128 characters.\n",
    "\n",
    "Some of you, however, will want to use other characters (e.g., Chinese\n",
    "characters). To solve this there is\n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode) which assigns numbers to\n",
    "symbols, e.g., 041 is `'A'` and 03A3 is `'Σ'` (numbers starting with 0 are\n",
    "hexadecimal). Often non/beyond-ASCII characters are called Unicode characters.\n",
    "Unicode contains 1,114,112 characters, about 10\\% of which have been assigned.\n",
    "Unfortunately there are many ways used to map combinations of bits to Unicode\n",
    "symbols. The ones you are likely to encounter are called by Python _utf-8_,\n",
    "_utf-16_ and _latin-1_. _utf-8_ is the standard for Linux and Mac OS while both\n",
    "_utf-16_ and _latin-1_ are used by windows. If you use the wrong encoding,\n",
    "characters can appear wrong, sometimes change in number or Python could raise an\n",
    "exception. Lets see what happens when we open the file we just created with\n",
    "different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with the correct encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n",
      "\n",
      "This is with the wrong encoding:\n",
      "A line\n",
      "Another line\n",
      "A line with a few unusual symbols â¡ â â  â¡ â¢ â£ àµ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with _latin-1_ the unicode characters are mixed up and there are too\n",
    "many of them. You need to keep in mind encoding when obtaining text files.\n",
    "Determining the encoding can sometime involve substantial work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load many text files at once. Let's start by looking at the Shakespeare files in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and Train.]\n",
      "\n",
      "PUCK\n",
      "  If we shadows have offended,\n",
      "  Think but this,--and all is mended,--\n",
      "  That you have but slumber'd here\n",
      "  While these visions did appear.\n",
      "  And this weak and idle theme,\n",
      "  No more yielding but a dream,\n",
      "  Gentles, do not reprehend;\n",
      "  If you pardon, we will mend.\n",
      "  And, as I am an honest Puck,\n",
      "  If we have unearned luck\n",
      "  Now to 'scape the serpent's tongue,\n",
      "  We will make amends ere long;\n",
      "  Else the Puck a liar call:\n",
      "  So, good night unto you all.\n",
      "  Give me your hands, if we be friends,\n",
      "  And Robin shall restore amends.\n",
      "\n",
      "[Exit.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "End of Project Gutenberg Etext of A Midsummer Night's Dream by Shakespeare\n",
      "PG has multiple editions of William Shakespeare's Complete Works\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Shakespeare/midsummer_nights_dream.txt') as f:\n",
    "    midsummer = f.read()\n",
    "print(midsummer[-700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to load all the files in `../data/Shakespeare` we can use a for loop with `scandir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targetDir = '../data/Shakespeare' #Change this to your own directory of texts\n",
    "shakespearText = []\n",
    "shakespearFileName = []\n",
    "\n",
    "for file in (file for file in os.scandir(targetDir) if file.is_file() and not file.name.startswith('.')):\n",
    "    with open(file.path) as f:\n",
    "        shakespearText.append(f.read())\n",
    "    shakespearFileName.append(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can put them all in pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alls_well_that_ends_well.txt</th>\n",
       "      <td>All's Well, that Ends Well\\n\\nActus primus. Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthonie_and_cleopatra.txt</th>\n",
       "      <td>The Tragedie of Anthonie, and Cleopatra\\n\\nAct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as_you_like_it.txt</th>\n",
       "      <td>AS YOU LIKE IT\\n\\nby William Shakespeare\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comedy_of_errors.txt</th>\n",
       "      <td>DRAMATIS PERSONAE\\n\\nSOLINUS, Duke of Ephesus\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coriolanus.txt</th>\n",
       "      <td>THE TRAGEDY OF CORIOLANUS\\n\\nby William Shakes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cymbeline.txt</th>\n",
       "      <td>The Tragedie of Cymbeline\\n\\nActus Primus. Sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamlet.txt</th>\n",
       "      <td>The Tragedie of Hamlet\\n\\nActus Primus. Scoena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>julius_caesar.txt</th>\n",
       "      <td>Dramatis Personae\\n\\n  JULIUS CAESAR, Roman st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_4_p1.txt</th>\n",
       "      <td>The First Part of Henry the Fourth\\n\\nwith the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_4_p2.txt</th>\n",
       "      <td>KING HENRY IV, SECOND PART\\n\\nby William Shake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_5.txt</th>\n",
       "      <td>THE LIFE OF KING HENRY THE FIFTH\\n\\nby William...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_6_p1.txt</th>\n",
       "      <td>Dramatis Personae\\n\\nKING HENRY the Sixth\\nDUK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_6_p2.txt</th>\n",
       "      <td>The second Part of Henry the Sixt\\n\\nwith the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_6_p3.txt</th>\n",
       "      <td>The third Part of Henry the Sixt\\n\\nwith the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_henry_8.txt</th>\n",
       "      <td>KING HENRY THE EIGHTH\\n\\nby William Shakespear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_john.txt</th>\n",
       "      <td>The life and death of King John\\n\\nActus Primu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_lear.txt</th>\n",
       "      <td>The Tragedie of King Lear\\n\\n\\nActus Primus. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_richard_2.txt</th>\n",
       "      <td>DRAMATIS PERSONAE\\n\\n  KING RICHARD THE SECOND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>king_richard_3.txt</th>\n",
       "      <td>KING RICHARD III\\n\\nby William Shakespeare\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lovers_complaint.txt</th>\n",
       "      <td>A LOVER'S COMPLAINT\\n\\nby William Shakespeare\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loves_labors_lost.txt</th>\n",
       "      <td>LOVE'S LABOUR'S LOST\\n\\nby William Shakespeare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macbeth.txt</th>\n",
       "      <td>MACBETH\\n\\nby William Shakespeare\\n\\n\\n\\n\\nPer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>measure_for_measure.txt</th>\n",
       "      <td>MEASURE FOR MEASURE\\n\\nby William Shakespeare\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merchant_of_venice.txt</th>\n",
       "      <td>The Merchant of Venice\\n\\nActus primus.\\n\\nEnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merry_wives_of_windsor.txt</th>\n",
       "      <td>THE MERRY WIVES OF WINDSOR\\n\\nby William Shake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midsummer_nights_dream.txt</th>\n",
       "      <td>A MIDSUMMER NIGHT'S DREAM\\n\\nby William Shakes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much_ado_about_nothing.txt</th>\n",
       "      <td>MUCH ADO ABOUT NOTHING\\n\\nby William Shakspere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>othello.txt</th>\n",
       "      <td>THE TRAGEDY OF OTHELLO, MOOR OF VENICE\\n\\nby W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passionate_pilgrim.txt</th>\n",
       "      <td>THE PASSIONATE PILGRIM\\n\\nby William Shakespea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pericles_prince_of_tyre.txt</th>\n",
       "      <td>PERICLES PRINCE OF TYRE\\n\\nby William Shakespe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phoenix_and_the_turtle.txt</th>\n",
       "      <td>THE PHOENIX AND THE TURTLE\\n\\nby William Shake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rape_of_lucrece.txt</th>\n",
       "      <td>THE RAPE OF LUCRECE\\n\\nby William Shakespeare\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romeo_and_juliet.txt</th>\n",
       "      <td>ROMEO AND JULIET\\n\\nby William Shakespeare\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sonnets.txt</th>\n",
       "      <td>THE SONNETS\\n\\nby William Shakespeare\\n\\n\\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taming_of_the_shrew.txt</th>\n",
       "      <td>THE TAMING OF THE SHREW\\n\\nby William Shakespe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempest.txt</th>\n",
       "      <td>The Tempest\\n\\nActus primus, Scena prima.\\n\\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timon_of_athens.txt</th>\n",
       "      <td>THE LIFE OF TIMON OF ATHENS\\n\\nby William Shak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titus_andronicus.txt</th>\n",
       "      <td>The Tragedie of Titus Andronicus\\n\\nActus Prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>troilus_and_cressida.txt</th>\n",
       "      <td>THE HISTORY OF TROILUS AND CRESSIDA\\n\\nby Will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twelth_night.txt</th>\n",
       "      <td>TWELFTH NIGHT;\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two_gentlemen_of_verona.txt</th>\n",
       "      <td>THE TWO GENTLEMEN OF VERONA\\n\\nby William Shak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venus_and_adonis.txt</th>\n",
       "      <td>VENUS AND ADONIS\\n\\nby William Shakespeare\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winters_tale.txt</th>\n",
       "      <td>THE WINTER'S TALE\\n\\nby William Shakespeare\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           text\n",
       "alls_well_that_ends_well.txt  All's Well, that Ends Well\\n\\nActus primus. Sc...\n",
       "anthonie_and_cleopatra.txt    The Tragedie of Anthonie, and Cleopatra\\n\\nAct...\n",
       "as_you_like_it.txt            AS YOU LIKE IT\\n\\nby William Shakespeare\\n\\n\\n...\n",
       "comedy_of_errors.txt          DRAMATIS PERSONAE\\n\\nSOLINUS, Duke of Ephesus\\...\n",
       "coriolanus.txt                THE TRAGEDY OF CORIOLANUS\\n\\nby William Shakes...\n",
       "cymbeline.txt                 The Tragedie of Cymbeline\\n\\nActus Primus. Sco...\n",
       "hamlet.txt                    The Tragedie of Hamlet\\n\\nActus Primus. Scoena...\n",
       "julius_caesar.txt             Dramatis Personae\\n\\n  JULIUS CAESAR, Roman st...\n",
       "king_henry_4_p1.txt           The First Part of Henry the Fourth\\n\\nwith the...\n",
       "king_henry_4_p2.txt           KING HENRY IV, SECOND PART\\n\\nby William Shake...\n",
       "king_henry_5.txt              THE LIFE OF KING HENRY THE FIFTH\\n\\nby William...\n",
       "king_henry_6_p1.txt           Dramatis Personae\\n\\nKING HENRY the Sixth\\nDUK...\n",
       "king_henry_6_p2.txt           The second Part of Henry the Sixt\\n\\nwith the ...\n",
       "king_henry_6_p3.txt           The third Part of Henry the Sixt\\n\\nwith the d...\n",
       "king_henry_8.txt              KING HENRY THE EIGHTH\\n\\nby William Shakespear...\n",
       "king_john.txt                 The life and death of King John\\n\\nActus Primu...\n",
       "king_lear.txt                 The Tragedie of King Lear\\n\\n\\nActus Primus. S...\n",
       "king_richard_2.txt            DRAMATIS PERSONAE\\n\\n  KING RICHARD THE SECOND...\n",
       "king_richard_3.txt            KING RICHARD III\\n\\nby William Shakespeare\\n\\n...\n",
       "lovers_complaint.txt          A LOVER'S COMPLAINT\\n\\nby William Shakespeare\\...\n",
       "loves_labors_lost.txt         LOVE'S LABOUR'S LOST\\n\\nby William Shakespeare...\n",
       "macbeth.txt                   MACBETH\\n\\nby William Shakespeare\\n\\n\\n\\n\\nPer...\n",
       "measure_for_measure.txt       MEASURE FOR MEASURE\\n\\nby William Shakespeare\\...\n",
       "merchant_of_venice.txt        The Merchant of Venice\\n\\nActus primus.\\n\\nEnt...\n",
       "merry_wives_of_windsor.txt    THE MERRY WIVES OF WINDSOR\\n\\nby William Shake...\n",
       "midsummer_nights_dream.txt    A MIDSUMMER NIGHT'S DREAM\\n\\nby William Shakes...\n",
       "much_ado_about_nothing.txt    MUCH ADO ABOUT NOTHING\\n\\nby William Shakspere...\n",
       "othello.txt                   THE TRAGEDY OF OTHELLO, MOOR OF VENICE\\n\\nby W...\n",
       "passionate_pilgrim.txt        THE PASSIONATE PILGRIM\\n\\nby William Shakespea...\n",
       "pericles_prince_of_tyre.txt   PERICLES PRINCE OF TYRE\\n\\nby William Shakespe...\n",
       "phoenix_and_the_turtle.txt    THE PHOENIX AND THE TURTLE\\n\\nby William Shake...\n",
       "rape_of_lucrece.txt           THE RAPE OF LUCRECE\\n\\nby William Shakespeare\\...\n",
       "romeo_and_juliet.txt          ROMEO AND JULIET\\n\\nby William Shakespeare\\n\\n...\n",
       "sonnets.txt                   THE SONNETS\\n\\nby William Shakespeare\\n\\n\\n\\n ...\n",
       "taming_of_the_shrew.txt       THE TAMING OF THE SHREW\\n\\nby William Shakespe...\n",
       "tempest.txt                   The Tempest\\n\\nActus primus, Scena prima.\\n\\nA...\n",
       "timon_of_athens.txt           THE LIFE OF TIMON OF ATHENS\\n\\nby William Shak...\n",
       "titus_andronicus.txt          The Tragedie of Titus Andronicus\\n\\nActus Prim...\n",
       "troilus_and_cressida.txt      THE HISTORY OF TROILUS AND CRESSIDA\\n\\nby Will...\n",
       "twelth_night.txt                                           TWELFTH NIGHT;\\n ...\n",
       "two_gentlemen_of_verona.txt   THE TWO GENTLEMEN OF VERONA\\n\\nby William Shak...\n",
       "venus_and_adonis.txt          VENUS AND ADONIS\\n\\nby William Shakespeare\\n\\n...\n",
       "winters_tale.txt              THE WINTER'S TALE\\n\\nby William Shakespeare\\n\\..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespear_df = pandas.DataFrame({'text' : shakespearText}, index = shakespearFileName)\n",
    "shakespear_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting your text in a format like this is the first step of most analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF\n",
    "\n",
    "Another common way text will be stored is in a PDF file. First we will download\n",
    "a pdf in Python. To do that lets grab a chapter from\n",
    "_Speech and Language Processing_, chapter 21 is on Information Extraction which\n",
    "seems apt. It is stored as a pdf at [https://web.stanford.edu/~jurafsky/slp3/21.\n",
    "pdf](https://web.stanford.edu/~jurafsky/slp3/21.pdf) although we are downloading\n",
    "from a copy just in case Jurafsky changes their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%PDF-1.3\n",
      "%���������\n",
      "4 0 obj\n",
      "<< /Length 5 0 R /Filter /FlateDecode >>\n",
      "stream\n",
      "x\u0001�]۶�F�}�W�c����T���\u0017C\u000f�i�\u0019<t\u001f�b\u0001\u000fM�f\n",
      "Tn�\u0006<3_�\u000b",
      "�CDf�\u001d",
      "�J�N�i�\u000f�#�%.;.\u0019���\t?\u000f߄��7�]8������ux��}\u001b޾\u000fm����y��bǾ���\u0010�!\u001c",
      "\u000e���$�Ǯ���C�\u0007�F\u0006�����p�\u000f��5��1��1�P<�{�\u0010$�\u001a�/$�P�\f",
      "s�v��P\u001e",
      "gH?�����Q�~�*�:l��ˇ�m�ǰ��C�l����܊\u0017��E��\u001e",
      "���\u000f!�^�y��\u001am�$�Ý���wۡل׼�6w���ī�K�~؞���r��\u0010~\u001b\u001e",
      "?�ˡkO�;6IH�9{ԡ���\u0000]?�E�E�\u0012�~���.l������+��\u001c",
      "W�\u000e\u0002_�\u000e��\u0002\u0002��C��S�|�~\u0005C��N�3ӛB`8�ޚ\b\u0001j9���AZ�\u0004�\u00110�d�l^�\u000e�����SY\u0012\t�Ƨ��>\u000b",
      "q�ۇ&\n",
      "����.�����0���\u0015�;\u0000��>a8�$\f",
      "w�p��p����ST���\u000b",
      ".\u0000�7��@�\u0012���)�\u0013�&1�|���\u0002\u0004WՃ jOv�G2b�L8I��N�@\u0001\u001e",
      "gǍ�\u0004����\u0019�O�C��������IN@@���\u0002��\u0013}�8��+L����a�\u0005&ү\b�o\u0005\u0013�V(\u0019���0\f",
      "���+5\u001b�\n",
      "S\u001d",
      "fS&��<�2���\u001e",
      "��>l�V��&��=4⇤\u0019=\u001a�W��<�J\u0013Mo�\u001c",
      "���\"����d�C����[vY�|K\u001c",
      "{_ܔ\\��\u0017��%\u0001H�/@'�QA�+D�l��c��L�G�.��\t�̎�V�:f>���Aw\u0010K���o$`D\u0007��\u000b",
      "bE45�\u000b",
      "0\b�\u0015%th6h��\u0005���>*�2vQd\u0010\u0015�+M��Y}�Q���u�[���N�o'b\u0010��/u�.r'Z�\u0017��J�\u0019e8�v\u0013\u000b",
      "��;�\u001d",
      "�{T�\t\f",
      "�����^8�\u0014 \u001a\u0018 l<�E�<���b�����C8\f",
      "j��f��xB>\u0001K\u0010���\u0019��|\u001f\u0004w��f�|?�\u0001s̭\u0018��Y�'�Ip&�\"�\u000b",
      "A���f�?�\b!IYi���U�\"��y;�\u0007��#�\u000b",
      "\u000f�e3)�+B�&���\u001d",
      "�<\bE9I�g�/]\"D��yfC;e����Y^�z ��s'�)/�X�-HY��<ˬ�ݰ\n"
     ]
    }
   ],
   "source": [
    "#information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says `'pdf'`, so thats a good sign. The rest though looks like we are having\n",
    "issues with an encoding. The random characters are not caused by our encoding\n",
    "being wrong, however. They are cause by there not being an encoding for those\n",
    "parts at all. PDFs are nominally binary files, meaning there are sections of\n",
    "binary that are specific to pdf and nothing else so you need something that\n",
    "knows about pdf to read them. To do that we will be using\n",
    "[`PyPDF2`](https://github.com/mstamy2/PyPDF2), a PDF processing library for\n",
    "Python 3.\n",
    "\n",
    "\n",
    "Because PDFs are a very complicated file format pdfminer requires a large amount\n",
    "of boilerplate code to extract text, we have written a function that takes in an\n",
    "open PDF file and returns the text so you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Based on code from http://stackoverflow.com/a/20905381/4955164\n",
    "    #Using utf-8, if there are a bunch of random symbols try changing this\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to take the response object and convert it into a 'file like'\n",
    "object so that pdfminer can read it. To do this we will use `io`'s `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give it to pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department of  Sociology \n",
      "\n",
      "THE UNIVERSITY OF CHICAGO \n",
      "\n",
      "SOCIOLOGY 40133 \n",
      "\n",
      "Computational Content Analysis \n",
      "\n",
      "Friday 1:00 – 3:50pm \n",
      "Winter 2017-2018 \n",
      "Classroom: Harper Memorial 130       \n",
      "http://chalk.uchicago.edu/ \n",
      "\n",
      " \n",
      "\n",
      "                                                                                           \n",
      "\n",
      "          Office: McGiffert 210 \n",
      "                                                    Tel.: 834-3612; jevans@uchicago.edu \n",
      "                                  Office Hours: Thursday 12:30-2:30pm \n",
      "\n",
      "     \n",
      "\n",
      "        James A. Evans            \n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(readPDF(infoExtractionBytes)[:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can either look at the full text or fiddle with our PDF reader and\n",
    "get more information about individual blocks of text.\n",
    "\n",
    "## Word Docs\n",
    "\n",
    "The other type of document you are likely to encounter is the `.docx`, these are\n",
    "actually a version of [XML](https://en.wikipedia.org/wiki/Office_Open_XML), just\n",
    "like HTML, and like HTML we will use a specialized parser.\n",
    "\n",
    "For this class we will use [`python-docx`](https://python-\n",
    "docx.readthedocs.io/en/latest/) which provides a nice simple interface for\n",
    "reading `.docx` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "\n",
      "Accessing the Research Computing Center Resources\n",
      "\n",
      "To connect to the midway compute cluster to access your home directory and the macs60000 storage space, and utilize the HPC resources, you will either use a terminal client (with or without X11 forwarding capabilities) or the Linux remote desktop server software client (Thinlinc) to connect to the midway cluster. To submit jobs, monitor jobs, browse directories or do other computing you will need to connect through either the terminal or remote desktop. Setup and utilization of these clients will be discussed below in the context of your local platform’s architecture.\n",
      "SSH Client Setup & Remote Desktop Server\n"
     ]
    }
   ],
   "source": [
    "#example_docx = 'https://github.com/KnowledgeLab/content_analysis/raw/data/example_doc.docx'\n",
    "\n",
    "r = requests.get(example_docx, stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure uses the `io.BytesIO` class again, since `docx.Document` expects\n",
    "a file. Another way to do it is to save the document to a file and then read it\n",
    "like any other file. If we do this we can either delete the file afterwords, or\n",
    "save it and avoid downloading the following time.\n",
    "\n",
    "This function is useful as a part of many different tasks so it and others like it will be added to the helper package `lucem_illud` so we can use it later without having to retype it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadIfNeeded(targetURL, outputFile, **openkwargs):\n",
    "    if not os.path.isfile(outputFile):\n",
    "        outputDir = os.path.dirname(outputFile)\n",
    "        #This function is a more general os.mkdir()\n",
    "        if len(outputDir) > 0:\n",
    "            os.makedirs(outputDir, exist_ok = True)\n",
    "        r = requests.get(targetURL, stream=True)\n",
    "        #Using a closure like this is generally better than having to\n",
    "        #remember to close the file. There are ways to make this function\n",
    "        #work as a closure too\n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return open(outputFile, **openkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will download, save and open `outputFile` as `outputFile` or just\n",
    "open it if `outputFile` exists. By default `open()` will open the file as read\n",
    "only text with the local encoding, which may cause issues if its not a text\n",
    "file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is not a zip file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d = docx.Document(downloadIfNeeded(example_docx, example_docx_save))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tell `open()` to read in binary mode (`'rb'`), this is why we added\n",
    "`**openkwargs`, this allows us to pass any keyword arguments (kwargs) from\n",
    "`downloadIfNeeded` to `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "\n",
      "Accessing the Research Computing Center Resources\n",
      "\n",
      "To connect to the midway compute cluster to access your home directory and the macs60000 storage space, and utilize the HPC resources, you will either use a terminal client (with or without X11 forwarding capabilities) or the Linux remote desktop server software client (Thinlinc) to connect to the midway cluster. To submit jobs, monitor jobs, browse directories or do other computing you will need to connect through either the terminal or remote desktop. Setup and utilization of these clients will be discussed below in the context of your local platform’s architecture.\n",
      "SSH Client Setup & Remote Desktop Server\n"
     ]
    }
   ],
   "source": [
    "d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb'))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the file with `docx.Document` and not have to wait for it to be\n",
    "downloaded every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">Section 3</span>\n",
    "<span style=\"color:red\">Construct cells immediately below this that extract and organize textual content from text, PDF or Word into a pandas dataframe.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographical Analysis ISSN 0016-7363\n",
      "\n",
      "GeoDa: An Introduction to Spatial Data\n",
      "Analysis\n",
      "\n",
      "Luc Anselin1, Ibnu Syabri2, Youngihn Kho1\n",
      "1Spatial Analysis Laboratory, Department of Geography, University of Illinois, Urbana, IL, 2Laboratory for\n",
      "Spatial Computing and Analysis, Department of Regional and City Planning, Institut Teknologi, Bandung,\n",
      "Indonesia\n",
      "\n",
      "This article presents an overview of GeoDaTM, a free software program intended to\n",
      "serve as a user-friendly and graphical\n",
      "introduction to spatial analysis for non-\n",
      "geographic information systems (GIS) specialists. It includes functionality ranging from\n",
      "simple mapping to exploratory data analysis, the visualization of global and local\n",
      "spatial autocorrelation, and spatial regression. A key feature of GeoDa is an interactive\n",
      "environment that combines maps with statistical graphics, using the technology of\n",
      "dynamically linked windows. A brief review of the software design is given, as well as\n",
      "some illustrative examples that highlight distinctive features of\n",
      "the program in\n",
      "applications dealing with public health, economic development, real estate analysis,\n",
      "and criminology.\n",
      "\n",
      "Introduction\n",
      "\n",
      "The development of specialized software for spatial data analysis has seen rapid\n",
      "growth as the lack of such tools was lamented in the late 1980s by Haining (1989)\n",
      "and cited as a major impediment to the adoption and use of spatial statistics by\n",
      "geographic information systems (GIS) researchers. Initially, attention tended to\n",
      "focus on conceptual issues, such as how to integrate spatial statistical methods and\n",
      "a GIS environment (loosely versus tightly coupled, embedded versus modular, etc.),\n",
      "and which techniques would be most fruitfully included in such a framework.\n",
      "Familiar reviews of these issues are represented in, among others, Anselin and Getis\n",
      "(1992); Goodchild et al. (1992); Fischer and Nijkamp (1993); Fotheringham and\n",
      "Rogerson (1993, 1994); Fischer, Scholten, and Unwin (1996); and Fischer and Getis\n",
      "(1997). Today, the situation is quite different, and a fairly substantial collection of\n",
      "spatial data analysis software is readily available, ranging from niche programs,\n",
      "customized scripts and extensions for commercial statistical and GIS packages, to a\n",
      "\n",
      "Correspondence: Luc Anselin, Department of Geography, University of Illinois, Urbana-\n",
      "Champaign, Urbana, IL 61801\n",
      "e-mail: anselin@uiuc.edu\n",
      "\n",
      "Submitted: January 1, 2004. Revised version accepted: March 10, 2005.\n",
      "\n",
      "Geographical Analysis 38 (2006) 5–22 r 2006 The Ohio State University\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "burgeoning open-source effort using software environments such as R, Java, and\n",
      "Python. This is exemplified by the growing contents of the software tools clearing\n",
      "house maintained by the U.S.-based Center for Spatially Integrated Social Science\n",
      "(CSISS).1\n",
      "\n",
      "CSISS was established in 1999 as a research infrastructure project funded by the\n",
      "U.S. National Science Foundation in order to promote a spatial analytical perspec-\n",
      "tive in the social sciences (Goodchild et al. 2000). It was readily recognized that a\n",
      "major instrument in disseminating and facilitating spatial data analysis would be an\n",
      "easy-to-use, visual, and interactive software package aimed at the non-GIS user and\n",
      "requiring as little as possible in terms of other software (such as GIS or statistical\n",
      "packages). GeoDa is the outcome of this effort. It is envisaged as an ‘‘introduction to\n",
      "spatial data analysis’’ where the latter is taken to consist of visualization, explora-\n",
      "tion, and explanation of interesting patterns in geographic data.\n",
      "\n",
      "The main objective of the software is to provide the user with a natural path\n",
      "through an empirical spatial data analysis exercise, starting with simple mapping\n",
      "and geovisualization, moving on to exploration, spatial autocorrelation analysis,\n",
      "and ending up with spatial regression. In many respects, GeoDa is a reinvention of\n",
      "the original SpaceStat package (Anselin 1992), which by now has become quite\n",
      "dated, with only a rudimentary user interface, an antiquated architecture, and\n",
      "performance constraints for medium and large data sets. The software was rede-\n",
      "signed and rewritten from scratch, around the central concept of dynamically\n",
      "linked graphics. This means that different ‘‘views’’ of the data are represented\n",
      "as graphs, maps, or tables with selected observations in one highlighted in all. In\n",
      "that respect, GeoDa is similar to a number of other modern spatial data analysis\n",
      "software tools, although it is quite distinct in its combination of user friendliness\n",
      "with an extensive range of incorporated methods. A few illustrative comparisons\n",
      "will help clarify its position in the current spatial analysis software landscape.\n",
      "\n",
      "In terms of the range of spatial statistical techniques included, GeoDa is most\n",
      "alike to the collection of functions developed in the open-source R environment.\n",
      "For example, descriptive spatial autocorrelation measures, rate smoothing, and\n",
      "spatial regression are included in the spdep package, as described by Bivand and\n",
      "Gebhardt (2000), Bivand (2002a,b), and Bivand and Portnov (2004). In contrast to\n",
      "R, GeoDa is completely driven by a point and click interface and does not require\n",
      "any programming. It also has more extensive mapping capability (still somewhat\n",
      "experimental in R) and full linking and brushing in dynamic graphics, which is\n",
      "currently not possible in R due to limitations in its architecture. On the other hand,\n",
      "GeoDa is not (yet) customizable or extensible by the user, which is one of the\n",
      "strengths of\n",
      "the two are seen as highly\n",
      "complementary, ideally with more sophisticated users ‘‘graduating’’ to R after\n",
      "being introduced to the techniques in GeoDa.2\n",
      "\n",
      "the R environment.\n",
      "\n",
      "In that sense,\n",
      "\n",
      "The use of dynamic linking and brushing as a central organizing technique for\n",
      "data visualization has a strong tradition in exploratory data analysis (EDA), going\n",
      "back to the notion of linked scatter plot brushing (Stuetzle 1987) and various\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "(1988).\n",
      "\n",
      "methods for dynamic graphics outlined in Cleveland and McGill\n",
      "In\n",
      "geographical analysis, the concept of ‘‘geographic brushing’’ was introduced by\n",
      "Monmonier (1989) and made operational in the Spider/Regard toolboxes of Haslett,\n",
      "Unwin, and associates (Haslett, Wills, and Unwin 1990; Unwin 1994). Several\n",
      "modern toolkits for exploratory spatial data analysis (ESDA) also incorporate\n",
      "dynamic linking, and, to a lesser extent, brushing. Some of these rely on interaction\n",
      "with a GIS for the map component, such as the linked frameworks combining\n",
      "XGobi or XploRe with ArcView (Cook et al. 1996, 1997; Symanzik et al. 2000);\n",
      "the SAGE toolbox, which uses ArcInfo (Wise, Haining, and Ma 2001); and the\n",
      "DynESDA extension for ArcView (Anselin 2000), GeoDa’s immediate predecessor.\n",
      "Linking in these implementations is constrained by the architecture of the GIS,\n",
      "which limits the linking process to a single map (in GeoDa, there is no limit on the\n",
      "number of linked maps). In this respect, GeoDa is similar to other freestanding\n",
      "modern implementations of ESDA, such as the cartographic data visualizer, or cdv\n",
      "(Dykes 1997), GeoVISTA Studio (Takatsuka and Gahegan 2002), and STARS (Rey\n",
      "and Janikas 2006). These all include functionality for dynamic linking, and to a\n",
      "lesser extent, brushing. They are built in open-source programming environments,\n",
      "such as Tkl/Tk (cdv), Java (GeoVISTA Studio), or Python (STARS) and thus easily\n",
      "extensible and customizable. In contrast, GeoDa is (still) a closed box, but of these\n",
      "packages it provides the most extensive and flexible form of dynamic linking and\n",
      "brushing for both graphs and maps.\n",
      "\n",
      "Common spatial autocorrelation statistics, such as Moran’s I and even the Local\n",
      "Moran, are increasingly part of spatial analysis software, ranging from CrimeStat\n",
      "(Levine 2006), to the spdep and DCluster packages available on the open-source\n",
      "comprehensive R archive network (CRAN),3 as well as commercial packages, such\n",
      "as the spatial statistics toolbox of the forthcoming release of ArcGIS 9.0 (ESRI 2004).\n",
      "However, at this point in time, none of these include the range and ease of\n",
      "construction of spatial weights or the capacity to carry out sensitivity analysis and\n",
      "visualization of these statistics contained in GeoDa. Apart from the R spdep\n",
      "package, Geoda is the only one to contain functionality for spatial regression\n",
      "modeling among the software mentioned here.\n",
      "\n",
      "A prototype version of the software (known as DynESDA) has been in limited\n",
      "circulation since early 2001 (Anselin, Syabri, and Smirnov 2002a; Anselin et al.\n",
      "2002b), but the first official release of a beta version of GeoDa occurred on\n",
      "February 5, 2002. The program is available for free and can be downloaded from\n",
      "the CSISS software tools Web site (http://geoda.uiuc.edu).The most recent version,\n",
      "0.9.5-i, was released in January 2003. The software has been well received for both\n",
      "teaching and research use and has a rapidly growing body of users. For example, by\n",
      "the fall of 2005, there were more than 8000 registered users, increasing at a rate of\n",
      "about 200 new users per month.\n",
      "\n",
      "In the remainder of the article, we first outline the design and briefly review the\n",
      "overall functionality of GeoDa. This is followed by a series of illustrative examples,\n",
      "highlighting features of the mapping and geovisualization capabilities, exploration\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "in multivariate EDA, spatial autocorrelation analysis, and spatial regression. The\n",
      "article closes with some comments regarding future directions in the development\n",
      "of the software.\n",
      "\n",
      "Design and functionality\n",
      "\n",
      "The design of GeoDa consists of an interactive environment that combines maps\n",
      "with statistical graphs, using the technology of dynamically linked windows. It is\n",
      "geared to the analysis of discrete geospatial data, that is, objects characterized by\n",
      "their location in space either as points (point coordinates) or polygons (polygon\n",
      "boundary coordinates). The current version adheres to ESRI’s shape file as the\n",
      "standard for storing spatial information. It contains functionality to read and write\n",
      "such files, as well as to convert ASCII text input files for point coordinates or\n",
      "boundary file coordinates to the shape file format. It uses ESRI’s MapObjects LT2\n",
      "technology for spatial data access, mapping, and querying. The analytical function-\n",
      "ality is implemented in a modular fashion, as a collection of C11 classes with\n",
      "associated methods.\n",
      "\n",
      "In broad terms, the functionality can be classified into six categories:\n",
      "\n",
      "spatial data manipulation and utilities: data input, output, and conversion,\n",
      "data transformation: variable transformations and creation of new variables,\n",
      "\n",
      "\u000f\n",
      "\u000f\n",
      "\u000f mapping: choropleth maps, cartogram and map animation,\n",
      "\u000f\n",
      "\u000f\n",
      "\n",
      "EDA: statistical graphics,\n",
      "spatial autocorrelation: global and local spatial autocorrelation statistics, with\n",
      "inference and visualization,\n",
      "spatial regression: diagnostics and maximum likelihood estimation of linear\n",
      "spatial regression models.\n",
      "\n",
      "\u000f\n",
      "\n",
      "The full set of functions is listed in Table 1 and is documented in detail in the\n",
      "\n",
      "GeoDa user’s guides (Anselin 2003, 2004)4\n",
      "\n",
      "The software implementation consists of two important components: the user\n",
      "interface and graphics windows on the one hand and the computational engine on\n",
      "the other hand. In the current version, all graphic windows are based on Microsoft\n",
      "Foundation Classes (MFC) and thus are limited to MS Windows platforms.5 In\n",
      "contrast, the computational engine (including statistical operations, randomization,\n",
      "and spatial regression) is pure C11 code and largely cross platform.\n",
      "\n",
      "The bulk of the graphical interface implements five basic classes of windows:\n",
      "histogram, box plot, scatter plot (including the Moran scatter plot), map, and grid\n",
      "(for the table selection and calculations). The choropleth maps, including the\n",
      "significance and cluster maps for the local indicators of spatial autocorrelation\n",
      "(LISA), are derived from MapObjects classes. Three additional types of maps were\n",
      "developed from scratch and do not use MapObjects:\n",
      "the map movie (map\n",
      "animation), the cartogram, and the conditional maps. The three-dimensional scatter\n",
      "plot is implemented with the OpenGL library.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "Table 1 GeoDa Functionality Overview\n",
      "\n",
      "Category\n",
      "\n",
      "Functions\n",
      "\n",
      "Spatial data\n",
      "\n",
      "Data\n",
      "transformation\n",
      "\n",
      "Mapping\n",
      "\n",
      "EDA\n",
      "\n",
      "Spatial\n",
      "\n",
      "autocorrelation\n",
      "\n",
      "data input from shape file (point, polygon)\n",
      "data input from text (to point or polygon shape)\n",
      "data output to text (data or shape file)\n",
      "create grid polygon shape file from text input\n",
      "centroid computation\n",
      "Thiessen polygons\n",
      "variable transformation (log, exp, etc.)\n",
      "queries, dummy variables (regime variables)\n",
      "variable algebra (addition, multiplication, etc.)\n",
      "spatial lag variable construction\n",
      "rate calculation and rate smoothing\n",
      "data table join\n",
      "generic quantile choropleth map\n",
      "standard deviational map\n",
      "percentile map\n",
      "outlier map (box map)\n",
      "circular cartogram\n",
      "map movie\n",
      "conditional maps\n",
      "smoothed rate map (EB, spatial smoother)\n",
      "excess rate map (standardized mortality rate, SMR)\n",
      "histogram\n",
      "box plot\n",
      "scatter plot\n",
      "parallel coordinate plot\n",
      "three-dimensional scatter plot\n",
      "conditional plot (histogram, box plot, scatter plot)\n",
      "spatial weights creation (rook, queen, distance, k-nearest)\n",
      "higher order spatial weights\n",
      "spatial weights characteristics (connectedness histogram)\n",
      "Moran scatter plot with inference bivariate Moran scatter plot with\n",
      "inference\n",
      "Moran scatter plot for rates (EB standardization)\n",
      "Local Moran significance map\n",
      "Local Moran cluster map\n",
      "bivariate Local Moran\n",
      "Local Moran for rates (EB standardization)\n",
      "\n",
      "Spatial regression OLS with diagnostics (e.g., LM test, Motan’s I)\n",
      "\n",
      "Maximum likelihood spatial lag model\n",
      "Maximum likelihood spatial error model\n",
      "predicted value map\n",
      "residual map\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "The functionality of GeoDa is invoked either through menu items or directly by\n",
      "clicking toolbar buttons, as illustrated in Fig. 1. A number of specific applications\n",
      "are highlighted in the following sections, focusing on some distinctive features of\n",
      "the software.\n",
      "\n",
      "Mapping and geovisualization\n",
      "\n",
      "The bulk of the mapping and geovisualization functionality consists of a collection\n",
      "of specialized choropleth maps, focused on highlighting outliers in the data, so-\n",
      "called box maps (Anselin 1999). In addition, considerable capability is included to\n",
      "deal with the intrinsic variance instability of rates, in the form of empirical Bayes\n",
      "(EB) or spatial smoothers.6 As mentioned in ‘‘Design and functionality,’’\n",
      "the\n",
      "mapping operations use the classes contained in ESRI’s MapObjects, extended\n",
      "with the capability for linking and brushing. GeoDa also includes a circular\n",
      "cartogram,7 map animation in the form of a map movie, and conditional maps.\n",
      "The latter are nine micro-choropleth maps constructed by conditioning on three\n",
      "intervals for two conditioning variables, using the principles outlined in Becker,\n",
      "Cleveland, and Shyu (1996) and Carr et al. (2002).8 In contrast to the traditional\n",
      "choropleth maps, the cartogram, map movie, and conditional maps do not use\n",
      "MapObjects classes, and were developed from scratch.\n",
      "\n",
      "We illustrate the rate smoothing procedure, outlier maps and linking opera-\n",
      "tions. The objective in this analysis is to identify locations that have elevated\n",
      "mortality rates and to assess the sensitivity of the designation as outlier to the effect\n",
      "of rate smoothing. Using data on prostate cancer mortality in 156 counties\n",
      "contained in the Appalachian Cancer Network (ACN) for the period 1993–97,\n",
      "\n",
      "Figure 1. The opening screen with menu items and toolbar buttons.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "we construct a box map by specifying the number of deaths as the numerator and\n",
      "the population as the denominator.9 The resulting map for the crude rates (i.e.,\n",
      "without any adjustments for differing age distributions or other relevant factors) is\n",
      "shown as the upper-left panel in Fig. 2. Three counties are identified as outliers and\n",
      "shown in dark red.10 These match the outliers selected in the box plot in the lower-\n",
      "left panel of the figure. The linking of all maps and graphs results in those counties\n",
      "also being cross-hatched on the maps.\n",
      "\n",
      "The upper-right panel in the figure represents a smoothed rate map, where the\n",
      "rates were transformed by means of an Empirical Bayes procedure to remove the\n",
      "effect of the varying population at risk. As a result, the original outliers are no\n",
      "longer, but a different county is identified as having elevated risk. Also, a lower\n",
      "outlier is found as well, shown as dark blue in the box map.11 Note that the upper\n",
      "outlier is barely distinguishable, due to the small area of the county in question.\n",
      "This is a common problem when working with admininistrative units. In order to\n",
      "remove the potentially misleading effect of area on the perception of interesting\n",
      "patterns, a circular cartogram is shown in the lower-right panel of Fig. 2, where the\n",
      "area of the circles is proportional to the value of the EB smoothed rate. The upper\n",
      "outlier is shown as a red circle, the lower outlier as a blue circle. The yellow circles\n",
      "are the counties that were outliers in the crude rate map, highlighted here as a result\n",
      "of linking with the other maps and graphs.12\n",
      "\n",
      "Figure 2. Linked box maps, box plot, and cartogram, raw and smoothed prostate cancer\n",
      "mortality rates.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "Multivariate EDA\n",
      "\n",
      "Multivariate exploratory data analysis is implemented in GeoDa through linking\n",
      "and brushing between a collection of statistical graphs. These include the usual\n",
      "histogram, box plot, and scatter plot, but also a parallel coordinate plot (PCP) and\n",
      "three-dimensional scatter plot, as well as conditional plots (conditional histogram,\n",
      "box plot, and scatter plot).\n",
      "\n",
      "We illustrate some of this functionality with an exploration of the relationships\n",
      "between economic growth and initial development, typical of the recent ‘‘spatial’’\n",
      "regional convergence literature (for an overview see Rey 2004). We use economic\n",
      "data over the period 1980–99 for 145 European regions, most of them at the NUTS\n",
      "II level of spatial aggregation, except for a few at the NUTS I level (for Luxembourg\n",
      "and the United Kingdom).13\n",
      "\n",
      "Fig. 3 illustrates the various linked plots and map. The left-hand panel contains\n",
      "a simple percentile map (GDP per capita in 1989), and a three-dimensional scatter\n",
      "plot (for the percent agricultural and manufacturing employment in 1989 as well as\n",
      "the GDP growth rate over the period 1980–99). In the top right-hand panel is a PCP\n",
      "for the growth rates in the two periods of interest (1980–89 and 1989–99) and the\n",
      "GDP per capita in the base year,\n",
      "the typical components of a convergence\n",
      "regression. In the bottom of the right-hand panel is a simple scatter plot of the\n",
      "growth rate in the full period (1980–99) on the base year GDP.\n",
      "\n",
      "Figure 3. Multivariate exploratory data analysis with linking and brushing.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "Both plots on the right-hand side illustrate the typical empirical phenomenon\n",
      "that higher GDP at the start of the period is associated with a lower growth rate.\n",
      "However, as demonstrated in the PCP (some of the lines suggest a positive relation\n",
      "between GDP and growth rate), the pattern is not uniform and there is a suggestion\n",
      "of heterogeneity. A further exploration of this heterogeneity can be carried out by\n",
      "brushing any one of these graphs. For example, in Fig. 3, a selection box in the\n",
      "three-dimensional scatter plot is moved around (brushing) which highlights the\n",
      "selected observations in the map (cross-hatched) and in the PCP, clearly showing\n",
      "opposite patterns in subsets of the selection. Furthermore, in the scatter plot, the\n",
      "slope of the regression line can be recalculated for a subset of the data without\n",
      "the selected locations, to assess the sensitivity of the slope to those observations. In\n",
      "the example shown here, the effect on convergence over the whole period is\n",
      "minimal (\u0000 0.147 versus \u0000 0.144), but other selections show a more pronounced\n",
      "effect. Further exploration of\n",
      "these patterns does suggest a degree of spatial\n",
      "heterogeneity in the convergence results (for a detailed investigation, see Le Gallo\n",
      "and Dall’erba 2003).\n",
      "\n",
      "Spatial autocorrelation analysis\n",
      "\n",
      "Spatial autocorrelation analysis includes tests and visualization of both global (test\n",
      "for clustering) and local (test for clusters) Moran’s I statistic. The global test is\n",
      "visualized by means of a Moran scatter plot (Anselin 1996), in which the slope of\n",
      "the regression line corresponds to Moran’s I. Significance is based on a permutation\n",
      "test. The traditional univariate Moran scatter plot has been extended to depict\n",
      "bivariate spatial autocorrelation as well, that is, the correlation between one\n",
      "variable at a location, and a different variable at the neighboring locations (Anselin,\n",
      "Syabri, and Smirnov 2002a). In addition, there also is an option to standardize rates\n",
      "for the potentially biasing effect of variance instability (see Assunc¸a˜o and Reis\n",
      "1999).\n",
      "\n",
      "Local analysis is based on the Local Moran statistic (Anselin 1995), visualized\n",
      "in the form of significance and cluster maps. It also includes several options for\n",
      "sensitivity analysis, such as changing the number of permutations (to as many as\n",
      "9999), rerunning the permutations several times, and changing the significance\n",
      "cutoff value. This provides an ad hoc approach to assess the sensitivity of the results\n",
      "to problems due to multiple comparisons (i.e., how stable is the indication of\n",
      "clusters or outliers when the significance barrier is lowered).\n",
      "\n",
      "The maps depict the locations with significant Local Moran statistics (LISA\n",
      "significance maps) and classify those locations by type of association (LISA cluster\n",
      "maps). Both types of maps are available for brushing and linking. In addition to\n",
      "these two maps, the standard output of a LISA analysis includes a Moran scatter plot\n",
      "and a box plot depicting the distribution of the local statistic. Similar to the Moran\n",
      "scatter plot, the LISA concept has also been extended to a bivariate setup and\n",
      "includes an option to standardize for variance instability of rates.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "The functionality for spatial autocorrelation analysis is rounded out by a range\n",
      "of operations to construct spatial weights, using either boundary files (contiguity\n",
      "based) or point locations (distance based). A connectivity histogram helps in\n",
      "identifying potential problems with the neighbor structure, such as ‘‘islands’’\n",
      "(locations without neighbors).\n",
      "\n",
      "We illustrate spatial autocorrelation analysis with a study of\n",
      "\n",
      "the spatial\n",
      "distribution of 692 house sales prices for 1997 in Seattle, WA. This is part of a\n",
      "broader investigation into the effect of subsidized housing on the real estate\n",
      "market.14 For the purposes of this example, we only focus on the univariate spatial\n",
      "distribution, and the location of any significant clusters or spatial outliers in the\n",
      "data.\n",
      "\n",
      "The original house sales data are for point locations, which, for the purposes of\n",
      "this analysis are converted to Thiessen polygons. This allows a definition of\n",
      "‘‘neighbor’’ based on common boundaries between the Thiessen polygons. On\n",
      "the left-hand panel of Fig. 4, two LISA cluster maps are shown, depicting the\n",
      "locations of significant Local Moran’s I statistics, classified by type of spatial\n",
      "association. The dark red and dark blue locations are indications of spatial clusters\n",
      "(respectively, high surrounded by high, and low surrounded by low).15 In contrast,\n",
      "the light red and light blue are indications of spatial outliers (respectively, high\n",
      "surrounded by low, and low surrounded by high). The bottom map uses the default\n",
      "\n",
      "Figure 4. Local indicators of spatial autocorrelation cluster maps and significance maps.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "significance of P 5 0.05, whereas the top map is based on P 5 0.01 (after carrying\n",
      "out 9999 permutations). The matching significance map is in the top right-hand\n",
      "panel of Fig. 4. Significance is indicated by darker shades of green, with the darkest\n",
      "corresponding to P 5 0.0001. Note how the tighter significance criterion elemi-\n",
      "nates some (but not that many) locations from the map. In the bottom right-hand\n",
      "panel of the figure, the corresponding Moran scatter plot is shown, with the most\n",
      "extreme ‘‘high–high’’ locations selected. These are shown as cross-hatched poly-\n",
      "gons in the maps, and almost all obtain highly significant (at P 5 0.0001) local\n",
      "Moran’s I statistics.\n",
      "\n",
      "The overall pattern depicts a cluster of high priced houses on the East side, with\n",
      "a cluster of low priced houses following an axis through the center. Put in context,\n",
      "this is not surprising as the East side represents houses with a lake view, while the\n",
      "center cluster follows a highway axis and generally corresponds with a lower\n",
      "income neighborhood. Interestingly, the pattern is not uniform, and several spatial\n",
      "outliers can be distinguished. Further investigation of these patterns would require a\n",
      "full hedonic regression analysis.\n",
      "\n",
      "Spatial regression\n",
      "\n",
      "As of version 0.9.5-i, GeoDa also includes a limited degree of spatial regression\n",
      "functionality. The basic diagnostics for spatial autocorrelation, heteroskedasticity\n",
      "and nonnormality, are implemented for the standard ordinary least-squares regres-\n",
      "sion. Estimation of the spatial lag and spatial error models is supported by means of\n",
      "the maximum likelihood (ML) method (see Anselin and Bera 1998, for a review of\n",
      "the technical issues). In addition to the estimation itself, predicted values and\n",
      "residuals are calculated and made available for mapping.\n",
      "\n",
      "The ML estimation in GeoDa distinguishes itself by the use of extremely\n",
      "efficient algorithms that allow the estimation of models for very large data sets.\n",
      "The standard eigenvalue simplification is used (Ord 1975) for data sets up to 1000\n",
      "observations. Beyond that, the sparse algorithm of Smirnov and Anselin (2001) is\n",
      "used, which exploits the characteristic polynomial associated with the spatial\n",
      "weights matrix. This algorithm allows estimation of very large data sets in reason-\n",
      "able time. In addition, GeoDa implements the recent algorithm of Smirnov (2003)\n",
      "to compute the asymptotic variance matrix for all the model coefficients (i.e.,\n",
      "including both the spatial and nonspatial coefficients). This involves the inversion\n",
      "of a matrix of the dimensions of the data sets. To date, GeoDa is the only software\n",
      "that provides such estimates for large data sets.\n",
      "\n",
      "All estimation methods employ sparse spatial weights, but they are currently\n",
      "constrained to weights that are intrinsically symmetric (e.g., excluding k-nearest\n",
      "neighbor weights). The regression routines have been successfully applied to real\n",
      "data sets of more than 300,000 observations (with estimation and inference\n",
      "completed in a few minutes). By comparison, a spatial regression for the 30001\n",
      "U.S. counties takes a few seconds.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "We illustrate the spatial regression capabilities with a partial replication and\n",
      "extension of the homicide model used in Baller et al. (2001) and Messner and\n",
      "Anselin (2004). These studies assessed the extent to which a classic regression\n",
      "specification, well-known in the criminology literature, is robust to the explicit\n",
      "consideration of spatial effects. The model relates county homicide rates to a\n",
      "number of socioeconomic explanatory variables. In the original study, a full ML\n",
      "analysis of all the U.S. continental counties was precluded by the constraints on the\n",
      "eigenvalue-based SpaceStat routines. Instead, attention focused on two subsets of\n",
      "the data containing 1412 counties in the U.S. South and 1673 counties in the non-\n",
      "South.\n",
      "\n",
      "In Fig. 5, we show the result of the ML estimation of a spatial error model of\n",
      "county homicide rates for the complete set of 3085 continental U.S. counties in\n",
      "1980. The explanatory variables are the same as before: a Southern dummy\n",
      "\n",
      "Figure 5. Maximum likelihood estimation of the spatial error model.\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "variable, a resource deprivation index, a population structure indicator, unemploy-\n",
      "ment rate, divorce rate, and median age.16\n",
      "The results confirm a strong positive and significant spatial autoregressive\n",
      "coefficient ð^l ¼ 0:29Þ. Relative to the OLS results (e.g., Messner and Anselin 2004,\n",
      "Table 7.1., p. 137), the coefficient for unemployment has become insignificant,\n",
      "illustrating the misleading effect spatial error autocorrelation may have on infer-\n",
      "ence using OLS estimates. The model diagnostics also suggest a continued presence\n",
      "of problems with heteroskedasticity. However, GeoDa currently does not include\n",
      "functionality to deal with this.\n",
      "\n",
      "Future directions\n",
      "\n",
      "GeoDa is a work in progress and still under active development. This development\n",
      "proceeds along three fronts. First and foremost is an effort to make the code cross-\n",
      "platform and open source. This requires considerable change in the graphical\n",
      "interface, moving from the MFC that are standard in the various MS Windows\n",
      "flavors, to a cross-platform alternative. The current efforts use wxWidgets,17 which\n",
      "operates on the same code base with a native GUI flavor in Windows, MacOSX and\n",
      "Linux/Unix. Making the code open source is currently precluded by the reliance on\n",
      "proprietary code in ESRI’s MapObjects. Moreover, this involves more than simply\n",
      "making the source code available, but entails considerable reorganization and\n",
      "streamlining of code (refactoring),\n",
      "to make it possible for the community to\n",
      "effectively participate in the development process.\n",
      "\n",
      "A second strand of development concerns the spatial regression functionality.\n",
      "While currently still fairly rudimentary, the inclusion of estimators other than ML\n",
      "and the extension to models for spatial panel data are in progress. Finally, the\n",
      "functionality for ESDA itself is being extended to data models other than the\n",
      "discrete locations in the ‘‘lattice’’ case. Specifically, exploratory variography is\n",
      "being added, as well as the exploration of patterns in flow data.\n",
      "\n",
      "Given its initial rate of adoption, there is a strong indication that GeoDa is\n",
      "indeed providing the ‘‘introduction to spatial data analysis’’ that makes it possible\n",
      "for growing numbers of social scientists to be exposed to an explicit spatial\n",
      "perspective. Future development of the software should enhance this capability\n",
      "and it is hoped that the move to an open source environment will involve an\n",
      "international community of like-minded developers in this venture.\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      "This research was supported in part by U.S. National Science Foundation Grant\n",
      "BCS-9978058, to the Center for Spatially Integrated Social Science (CSISS) and by\n",
      "grant RO1 CA 95949-01 from the National Cancer Institute. In addition, this\n",
      "research was made possible in part through a Cooperative Agreement between\n",
      "the Center for Disease Control and Prevention (CDC) and the Association of\n",
      "Teachers of Preventive Medicine (ATPM), award number TS-1125. The contents\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "of the article are the responsibility of the authors and do not necessarily reflect the\n",
      "official views of NSF, NCI, the CDC, or ATPM. Special thanks go to Oleg Smirnov\n",
      "for his assistance with the implementation of the spatial regression routines and to\n",
      "Julie Le Gallo and Julia Koschinsky for preparing the data set for the European\n",
      "convergence study and for the Seattle house prices, respectively. GeoDaTM is a\n",
      "trademark of Luc Anselin.\n",
      "\n",
      "Notes\n",
      "\n",
      "1 See http://www.csiss.org/clearinghouse/\n",
      "2 Note that the CSISS spatial tools project is an active participant in the development of\n",
      "\n",
      "spatial data analysis methods in R, see, for example, http://sal.uiuc.edu/csiss/Rgeo/\n",
      "\n",
      "3 http://cran.r-project.org/\n",
      "4 A Quicktime movie with a demonstration of the main features can be found at http://\n",
      "\n",
      "sal.uiuc.edu/movies/GeoDaDemo.mov\n",
      "\n",
      "5 Ongoing development concerns the porting of all MFC-based classes to a cross-platform\n",
      "\n",
      "architecture, using wxWidgets. See also ‘‘Future directions’’ at the end of this article.\n",
      "6 The EB procedure is due to Clayton and Kaldor (1987); see also Marshall (1991) and\n",
      "\n",
      "Bailey and Gatrell (1995, pp. 303–08). For an alternative recent software\n",
      "implementation, see Anselin, Kim, and Syabri (2004). Spatial smoothing is discussed at\n",
      "length in Kafadar (1996).\n",
      "\n",
      "7 The cartogram is constructed using the nonlinear cellular automata algorithm due to\n",
      "\n",
      "Dorling (1996).\n",
      "\n",
      "8 The conditional maps are part of a larger set of conditional plots, which includes\n",
      "\n",
      "histograms, box plots, and scatter plots.\n",
      "\n",
      "9 Data obtained from the National Cancer Institute SEER (Surveillance, Epidemiology, and\n",
      "\n",
      "End Results) web site, http://seer.cancer.gov/seerstat/\n",
      "\n",
      "10 The respective counties are Cumberland, KY, Pocahontas, WV, and Forest, PA.\n",
      "11 The new upper outlier is Ohio county, WV; the lower outlier is Centre county, PA.\n",
      "12 Note that the outliers identified may be misleading as the rate analyzed is not adjusted for\n",
      "differences in age distribution. In other words, the outliers shown may simply be counties\n",
      "with a larger proportion of older males. A much more detailed analysis is necessary\n",
      "before any policy conclusions may be drawn.\n",
      "\n",
      "13 The data are from the most recent version of the NewCronos Regio database by Eurostat.\n",
      "\n",
      "NUTS stands for ‘‘Nomenclature of Territorial Units for Statistics’’ and contains the\n",
      "definition of administrative regions in the EU member states. NUTS II level regions are\n",
      "roughly comparable with counties in the U.S. context and are available for all but two\n",
      "countries. Luxembourg constitutes only a single region. For the United Kingdom, data is\n",
      "not available at the NUTS II level, as these regions do not correspond to local\n",
      "governmental units.\n",
      "\n",
      "14 The data are from the King County (Washington State) Department of Assessments.\n",
      "15 More precisely, the locations highlighted show the ‘‘core’’ of a cluster. The cluster itself\n",
      "can be thought of as consisting of the core as well as the neighbors. Clearly some of these\n",
      "clusters are overlapping.\n",
      "\n",
      "16 See the original articles for technical details and data sources. In Baller et al. (2001), a\n",
      "different set of spatial weights was used than in this example, but the conclusions of the\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "specification tests are the same. Specifically, using the county contiguity, the robust\n",
      "Lagrange Multiplier tests are 1.24 for the Lag alternative, and 24.88 for the Error\n",
      "alternative, strongly suggesting the latter as the proper alternative.\n",
      "\n",
      "17 http://www.wxwidgets.org\n",
      "\n",
      "References\n",
      "\n",
      "Anselin, L. (1992). SpaceStat, a Software Program for Analysis of Spatial Data. Santa Barbara,\n",
      "CA: National Center for Geographic Information and Analysis (NCGIA), University of\n",
      "California.\n",
      "\n",
      "Anselin, L. (1995). ‘‘Local Indicators of Spatial Association—LISA.’’ Geographical Analysis\n",
      "\n",
      "27, 93–115.\n",
      "\n",
      "Anselin, L. (1996). ‘‘The Moran Scatterplot as an ESDA Tool to Assess Local Instability in\n",
      "Spatial Association.’’ In Spatial Analytical Perspectives on GIS in Environmental and\n",
      "Socio-Economic Sciences, 111–25, edited by M. Fischer, H. Scholten, and D. Unwin.\n",
      "London: Taylor and Francis.\n",
      "\n",
      "Anselin, L. (1999). ‘‘Interactive Techniques and Exploratory Spatial Data Analysis.’’ In\n",
      "\n",
      "Geographical Information Systems: Principles, Techniques, Management and\n",
      "Applications, 251–64, edited by P. A. Longley, M. F. Goodchild, D. J. Maguire, and\n",
      "D. W. Rhind. New York: Wiley.\n",
      "\n",
      "Anselin, L. (2000). ‘‘Computing Environments for Spatial Data Analysis.’’ Journal of\n",
      "\n",
      "Geographical Systems 2(3), 201–20.\n",
      "\n",
      "Anselin, L. (2003). GeoDa 0.9 User’s Guide. Urbana-Champaign, IL: Spatial Analysis\n",
      "\n",
      "Laboratory (SAL), Department of Agricultural and Consumer Economics, University of\n",
      "Illinois.\n",
      "\n",
      "Anselin, L. (2004). GeoDa 0.95i Release Notes. Urbana-Champaign, IL: Spatial Analysis\n",
      "\n",
      "Laboratory (SAL), Department of Agricultural and Consumer Economics, University of\n",
      "Illinois.\n",
      "\n",
      "Anselin, L., and A. Bera. (1998). ‘‘Spatial Dependence in Linear Regression Models with an\n",
      "\n",
      "Introduction to Spatial Econometrics.’’ In Handbook of Applied Economic Statistics,\n",
      "237–89, edited by A. Ullah and D. E. Giles. New York: Marcel Dekker.\n",
      "\n",
      "Anselin, L., and A. Getis. (1992). ‘‘Spatial Statistical Analysis and Geographic Information\n",
      "\n",
      "Systems.’’ The Annals of Regional Science 26, 19–33.\n",
      "\n",
      "Anselin, L., Y.-W. Kim, and I. Syabri. (2004). ‘‘Web-Based Analytical Tools for the\n",
      "\n",
      "Exploration of Spatial Data.’’ Journal of Geographical Systems 6, 197–218.\n",
      "\n",
      "Anselin, L., I. Syabri, and O. Smirnov. (2002a). ‘‘Visualizing Multivariate Spatial Correlation\n",
      "\n",
      "with Dynamically Linked Windows.’’ In New Tools for Spatial Data Analysis:\n",
      "Proceedings of the Specialist Meeting, edited by L. Anselin and S. Rey. Santa Barbara,\n",
      "CA: Center for Spatially Integrated Social Science, University of California, CD-ROM.\n",
      "Anselin, L., I. Syabri, O. Smirnov, and Y. Ren. (2002b). ‘‘Visualizing Spatial Autocorrelation\n",
      "with Dynamically Linked Windows.’’ Computing Science and Statistics 33, CD-ROM.\n",
      "Assunc¸a˜o, R., and E. A. Reis. (1999). ‘‘A New Proposal to Adjust Moran’s I for Population\n",
      "\n",
      "Density.’’ Statistics in Medicine 18, 2147–61.\n",
      "\n",
      "Bailey, T. C., and A. C. Gatrell. (1995). Interactive Spatial Data Analysis. New York: Wiley.\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "Baller, R., L. Anselin, S. Messner, G. Deane, and D. Hawkins. (2001). ‘‘Structural Covariates\n",
      "\n",
      "of U.S. County Homicide Rates: Incorporating Spatial Effects.’’ Criminology 39(3),\n",
      "561–90.\n",
      "\n",
      "Becker, R. A., W. Cleveland, and M.-J. Shyu. (1996). ‘‘The Visual Design and Control of\n",
      "\n",
      "Trellis Displays.’’ Journal of Computational and Graphical Statistics 5, 123–55.\n",
      "\n",
      "Bivand, R. (2002a). ‘‘Implementing Spatial Data Analysis Software Tools in R.’’ In New Tools\n",
      "for Spatial Data Analysis: Proceedings of the Specialist Meeting, edited by L. Anselin and\n",
      "S. Rey. Santa Barbara, CA: Center for Spatially Integrated Social Science, University of\n",
      "California, CD-ROM.\n",
      "\n",
      "Bivand, R. (2002b). ‘‘Spatial Econometrics Functions in R: Classes and Methods.’’ Journal of\n",
      "\n",
      "Geographical Systems 4(4), 405–21.\n",
      "\n",
      "Bivand, R., and A. Gebhardt. (2000). ‘‘Implementing Functions for Spatial Statistical Analysis\n",
      "\n",
      "Using the R Language.’’ Journal of Geographical Systems 2(3), 307–17.\n",
      "\n",
      "Bivand, R. S., and B. A. Portnov. (2004). ‘‘Exploring Spatial Data Analysis Techniques Using\n",
      "R: The Case of Observations with No Neighbors.’’ In Advances in Spatial Econometrics:\n",
      "Methodology, Tools and Applications, 121–42, edited by L. Anselin, R. J. Florax, and S.\n",
      "J. Rey. Berlin: Springer-Verlag.\n",
      "\n",
      "Carr, D. B., J. Chen, S. Bell, L. Pickle, and Y. Zhang. (2002). ‘‘Interactive Linked Micromap\n",
      "Plots and Dynamically Conditioned Choropleth Maps.’’ In New Tools for Spatial Data\n",
      "Analysis: Proceedings of the Specialist Meeting, edited by L. Anselin and S. Rey. Santa\n",
      "Barbara, CA: Center for Spatially Integrated Social Science, University of California, CD-\n",
      "ROM.\n",
      "\n",
      "Clayton, D., and J. Kaldor. (1987). ‘‘Empirical Bayes Estimates of Age-Standardized Relative\n",
      "\n",
      "Risks for Use in Disease Mapping.’’ Biometrics 43, 671–81.\n",
      "\n",
      "Cleveland, W. S., and M. McGill. (1988). Dynamic Graphics for Statistics. Pacific Grove, CA:\n",
      "\n",
      "Wadsworth.\n",
      "\n",
      "Cook, D., J. Majure, J. Symanzik, and N. Cressie. (1996). ‘‘Dynamic Graphics in a GIS: A\n",
      "\n",
      "Platform for Analyzing and Exploring Multivariate Spatial Data.’’ Computational\n",
      "Statistics 11, 467–80.\n",
      "\n",
      "Cook, D., J. Symanzik, J. J. Majure, and N. Cressie. (1997). ‘‘Dynamic Graphics in a GIS:\n",
      "\n",
      "More Examples Using Linked Software.’’ Computers and Geosciences 23, 371–85.\n",
      "Dorling, D. (1996). Area Cartograms: Their Use and Creation. CATMOG 59, Institute of\n",
      "\n",
      "British Geographers.\n",
      "\n",
      "Dykes, J. A. (1997). ‘‘Exploring Spatial Data Representation with Dynamic Graphics.’’\n",
      "\n",
      "Computers and Geosciences 23, 345–70.\n",
      "\n",
      "ESRI. (2004). An Overview of the Spatial Statistics Toolbox. ArcGIS 9.0 Online Help System\n",
      "\n",
      "(ArcGIS 9.0 Desktop, Release 9.0, June 2004). Redlands, CA: Environmental Systems\n",
      "Research Institute.\n",
      "\n",
      "Fischer, M., and P. Nijkamp. (1993). Geographic Information Systems, Spatial Modelling and\n",
      "\n",
      "Policy Evaluation. Berlin: Springer-Verlag.\n",
      "\n",
      "Fischer, M. M., and A. Getis. (1997). Recent Development in Spatial Analysis. Berlin:\n",
      "\n",
      "Springer-Verlag.\n",
      "\n",
      "Fischer, M. M., H. J. Scholten, and D. Unwin. (1996). Spatial Analytical Perspectives on GIS.\n",
      "\n",
      "London: Taylor and Francis.\n",
      "\n",
      "Fotheringham, A. S., and P. Rogerson. (1993). ‘‘GIS and Spatial Analytical Problems.’’\n",
      "\n",
      "International Journal of Geographical Information Systems 7, 3–19.\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Luc Anselin et al.\n",
      "\n",
      "Spatial Data Analysis\n",
      "\n",
      "Fotheringham, A. S., and P. Rogerson. (1994). Spatial Analysis and GIS. London: Taylor and\n",
      "\n",
      "Francis.\n",
      "\n",
      "Goodchild, M. F., L. Anselin, R. Appelbaum, and B. Harthorn. (2000). ‘‘Toward\n",
      "\n",
      "Spatially Integrated Social Science.’’ International Regional Science Review 23(2),\n",
      "139–59.\n",
      "\n",
      "Goodchild, M. F., R. P. Haining, and S. Wise.\n",
      "\n",
      "(1992). ‘‘Integrating GIS and Spatial\n",
      "\n",
      "Analysis—Problems and Possibilities.’’ International Journal of Geographical\n",
      "Information Systems 6, 407–23.\n",
      "\n",
      "Haining, R. (1989). ‘‘Geography and Spatial Statistics: Current Positions, Future\n",
      "\n",
      "Developments.’’ In Remodelling Geography, 191–203, edited by B. Macmillan. Oxford,\n",
      "UK: Basil Blackwell.\n",
      "\n",
      "Haslett, J., G. Wills, and A. Unwin. (1990). ‘‘SPIDER—An Interactive Statistical Tool for the\n",
      "Analysis of Spatially Distributed Data.’’ International Journal of Geographic Information\n",
      "Systems 4, 285–96.\n",
      "\n",
      "Kafadar, K. (1996). ‘‘Smoothing Geographical Data, Particularly Rates of Disease.’’ Statistics\n",
      "\n",
      "in Medicine 15, 2539–60.\n",
      "\n",
      "Le Gallo, J., and S. Dall’erba. (2003). ‘‘Evaluating the Temporal and Spatial Heterogeneity\n",
      "\n",
      "of the European Convergence Process, 1980–1999.’’ Technical report, Universite´\n",
      "Montesquieu-Bordeaux IV, Pessac Cedex, France.\n",
      "\n",
      "Levine, N. (2006). ‘‘Crime Mapping and the CrimeStat Program: Geographical Analysis, 38,\n",
      "\n",
      "41–56.\n",
      "\n",
      "Marshall, R. J. (1991). ‘‘Mapping Disease and Mortality Rates Using Empirical Bayes\n",
      "\n",
      "Estimators.’’ Applied Statistics 40, 283–94.\n",
      "\n",
      "Messner, S. F., and L. Anselin. (2004). ‘‘Spatial Analyses of Homicide with Areal Data.’’ In\n",
      "\n",
      "Spatially Integrated Social Science, 127–44, edited by M. Goodchild and D. Janelle.\n",
      "New York: Oxford University Press.\n",
      "\n",
      "Monmonier, M. (1989). ‘‘Geographic Brushing: Enhancing Exploratory Analysis of the\n",
      "\n",
      "Scatterplot Matrix.’’ Geographical Analysis 21, 81–84.\n",
      "\n",
      "Ord, J. K. (1975). ‘‘Estimation Methods for Models of Spatial Interaction.’’ Journal of the\n",
      "\n",
      "American Statistical Association 70, 120–26.\n",
      "\n",
      "Rey, S. J. (2006). ‘‘Spatial Analysis of Regional Income Inequality.’’ In Spatially Integrated\n",
      "\n",
      "Social Science, 280–99, edited by M. F. Goodchild and D. Janelle. Oxford, UK: Oxford\n",
      "University Press.\n",
      "\n",
      "Rey, S. J., and M. V. Janikas. (2006). ‘‘STARS: Space–Time Analysis of Regional Systems.’’\n",
      "\n",
      "Geographical Analysis, 38, 67–86.\n",
      "\n",
      "Smirnov, O. (2003). ‘‘Computation of the Information Matrix for Models of Spatial\n",
      "\n",
      "Interaction.’’ Technical report, Regional Economics Applications Laboratory (REAL),\n",
      "University of Illinois, Urbana-Champaign.\n",
      "\n",
      "Smirnov, O., and L. Anselin. (2001). ‘‘Fast Maximum Likelihood Estimation of Very Large\n",
      "\n",
      "Spatial Autoregressive Models: A Characteristic Polynomial Approach.’’ Computational\n",
      "Statistics and Data Analysis 35, 301–19.\n",
      "\n",
      "Stuetzle, W. (1987). ‘‘Plot Windows.’’ Journal of the American Statistical Association 82,\n",
      "\n",
      "466–75.\n",
      "\n",
      "Symanzik, J., D. Cook, N. Lewin-Koh, J. J. Majure, and I. Megretskaia. (2000). ‘‘Linking\n",
      "ArcView and XGobi: Insight Behind the Front End.’’ Journal of Computational and\n",
      "Graphical Statistics 9(3), 470–90.\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Geographical Analysis\n",
      "\n",
      "Takatsuka, M., and M. Gahegan. (2002). ‘‘GeoVISTA Studio: A Codeless Visual Programming\n",
      "\n",
      "Environment for Geoscientific Data Analysis and Visualization.’’ Computers and\n",
      "Geosciences 28, 1131–41.\n",
      "\n",
      "Unwin, A. (1994). ‘‘REGARDing Geographic Data.’’ In Computational Statistics, 345–54,\n",
      "\n",
      "edited by P. Dirschedl and R. Osterman. Heidelberg: Physica Verlag.\n",
      "\n",
      "Wise, S., R. Haining, and J. Ma. (2001). ‘‘Providing Spatial Statistical Data Analysis\n",
      "\n",
      "Functionality for the GIS User: The SAGE Project.’’ International Journal of Geographic\n",
      "Information Science 15(3), 239–54.\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# I extracted and organized paragraphs from ga_geoda.pdf into a dataframe\n",
    "## Extract all the textual content\n",
    "with open('ga_geoda.pdf', 'rb') as f:\n",
    "    pdf_content = readPDF(f)\n",
    "    print(pdf_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split the text into paragraphs\n",
    "paras = []\n",
    "pdf_para = pdf_content.split('\\n\\n')\n",
    "for para in pdf_para:\n",
    "    ## Replace \"\\n\" with space\n",
    "    para = para.replace('\\n', ' ')\n",
    "    para = para.replace('\\x0f', ' ')\n",
    "    para = para.replace('\\x00', '')\n",
    "    para = para.replace('\\x0c', '')\n",
    "    ## Get rid of headers, footers, and page numbers\n",
    "    int_list = range(5, 23)\n",
    "    str_list = [str(i) for i in int_list]\n",
    "    if para != 'Geographical Analysis ISSN 0016-7363'\\\n",
    "    and para != 'Geographical Analysis 38 (2006) 5–22 r 2006 The Ohio State University'\\\n",
    "    and para != 'Geographical Analysis'\\\n",
    "    and para != ''\\\n",
    "    and para not in str_list:\n",
    "        paras.append(para)\n",
    "pdf_parasDF = pandas.DataFrame({'paragraph': paras})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             paragraph\n",
      "0      GeoDa: An Introduction to Spatial Data Analysis\n",
      "1    Luc Anselin1, Ibnu Syabri2, Youngihn Kho1 1Spa...\n",
      "2    This article presents an overview of GeoDaTM, ...\n",
      "3                                         Introduction\n",
      "4    The development of specialized software for sp...\n",
      "5    Correspondence: Luc Anselin, Department of Geo...\n",
      "6    Submitted: January 1, 2004. Revised version ac...\n",
      "7    burgeoning open-source effort using software e...\n",
      "8    CSISS was established in 1999 as a research in...\n",
      "9    The main objective of the software is to provi...\n",
      "10   In terms of the range of spatial statistical t...\n",
      "11                                  the R environment.\n",
      "12                                      In that sense,\n",
      "13   The use of dynamic linking and brushing as a c...\n",
      "14                                  Luc Anselin et al.\n",
      "15                               Spatial Data Analysis\n",
      "16                                             (1988).\n",
      "17   methods for dynamic graphics outlined in Cleve...\n",
      "18   Common spatial autocorrelation statistics, suc...\n",
      "19   A prototype version of the software (known as ...\n",
      "20   In the remainder of the article, we first outl...\n",
      "21   in multivariate EDA, spatial autocorrelation a...\n",
      "22                            Design and functionality\n",
      "23   The design of GeoDa consists of an interactive...\n",
      "24   In broad terms, the functionality can be class...\n",
      "25   spatial data manipulation and utilities: data ...\n",
      "26         mapping: choropleth maps, cartogram and ...\n",
      "27   EDA: statistical graphics, spatial autocorrela...\n",
      "28                                                    \n",
      "29   The full set of functions is listed in Table 1...\n",
      "..                                                 ...\n",
      "186                           in Medicine 15, 2539–60.\n",
      "187  Le Gallo, J., and S. Dall’erba. (2003). ‘‘Eval...\n",
      "188  of the European Convergence Process, 1980–1999...\n",
      "189  Levine, N. (2006). ‘‘Crime Mapping and the Cri...\n",
      "190                                             41–56.\n",
      "191  Marshall, R. J. (1991). ‘‘Mapping Disease and ...\n",
      "192       Estimators.’’ Applied Statistics 40, 283–94.\n",
      "193  Messner, S. F., and L. Anselin. (2004). ‘‘Spat...\n",
      "194  Spatially Integrated Social Science, 127–44, e...\n",
      "195  Monmonier, M. (1989). ‘‘Geographic Brushing: E...\n",
      "196  Scatterplot Matrix.’’ Geographical Analysis 21...\n",
      "197  Ord, J. K. (1975). ‘‘Estimation Methods for Mo...\n",
      "198       American Statistical Association 70, 120–26.\n",
      "199  Rey, S. J. (2006). ‘‘Spatial Analysis of Regio...\n",
      "200  Social Science, 280–99, edited by M. F. Goodch...\n",
      "201  Rey, S. J., and M. V. Janikas. (2006). ‘‘STARS...\n",
      "202                  Geographical Analysis, 38, 67–86.\n",
      "203  Smirnov, O. (2003). ‘‘Computation of the Infor...\n",
      "204  Interaction.’’ Technical report, Regional Econ...\n",
      "205  Smirnov, O., and L. Anselin. (2001). ‘‘Fast Ma...\n",
      "206  Spatial Autoregressive Models: A Characteristi...\n",
      "207  Stuetzle, W. (1987). ‘‘Plot Windows.’’ Journal...\n",
      "208                                            466–75.\n",
      "209  Symanzik, J., D. Cook, N. Lewin-Koh, J. J. Maj...\n",
      "210  Takatsuka, M., and M. Gahegan. (2002). ‘‘GeoVI...\n",
      "211  Environment for Geoscientific Data Analysis an...\n",
      "212  Unwin, A. (1994). ‘‘REGARDing Geographic Data....\n",
      "213  edited by P. Dirschedl and R. Osterman. Heidel...\n",
      "214  Wise, S., R. Haining, and J. Ma. (2001). ‘‘Pro...\n",
      "215  Functionality for the GIS User: The SAGE Proje...\n",
      "\n",
      "[216 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pdf_parasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
